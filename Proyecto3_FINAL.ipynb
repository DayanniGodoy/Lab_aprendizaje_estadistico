{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3a232f",
   "metadata": {},
   "source": [
    "# P03 - Final\n",
    "- Dayanni Godoy Rosales\n",
    "\n",
    "- Mateo Tavares Trueba\n",
    "\n",
    "- Jesús Flores Cortes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2080fe4",
   "metadata": {},
   "source": [
    "## Objetivo General\n",
    "\n",
    "Predecir con claridad la popularidad de la publicación con un dataset que contiene caracteríisticas de notas periodisticas y su respectivo alcance en redes sociales.\n",
    "\n",
    "Evaluar el desempeño predictivo de modelos avanzados de aprendizaje automático —Random Forest y XGBoost, tanto en clasificación como en regresión— utilizando validación cruzada estratificada (Stratified K-Folds), con el fin de identificar la capacidad real de las variables disponibles para explicar y predecir el fenómeno de interés.\n",
    "\n",
    "## Objetivos Específicos\n",
    "\n",
    "- Aplicar la técnica de Stratified K-Fold Cross-Validation para garantizar una evaluación robusta, estable y representativa del desempeño de los modelos, preservando la proporción de clases en cada partición del conjunto de datos.\n",
    "\n",
    "- Entrenar y evaluar un modelo Random Forest de regresión, con el propósito de estimar la capacidad explicativa de las características y determinar hasta qué punto los patrones no lineales permiten predecir la variable objetivo continua.\n",
    "\n",
    "- Entrenar y evaluar un modelo XGBoost de regresión, optimizando sus hiperparámetros para comparar su desempeño frente al Random Forest y analizar si técnicas de boosting mejoran la precisión en escenarios de alta complejidad o ruido.\n",
    "\n",
    "- Entrenar y evaluar un modelo Random Forest de clasificación, con el objetivo de identificar la habilidad del algoritmo para discriminar entre clases utilizando métricas robustas como ROC AUC y accuracy.\n",
    "\n",
    "- Entrenar y evaluar un modelo XGBoost de clasificación, buscando determinar si los métodos de boosting logran un mejor rendimiento que los métodos de bagging en la identificación de patrones no lineales y relaciones complejas entre las variables.\n",
    "\n",
    "- Comparar de manera sistemática los resultados de regresión y clasificación, con el fin de determinar cuál enfoque predictivo se ajusta mejor al comportamiento real de los datos y si la naturaleza del fenómeno favorece estimaciones continuas o categóricas.\n",
    "\n",
    "- Analizar la estabilidad de los modelos mediante la revisión de la desviación estándar de las métricas obtenidas durante la validación cruzada, para evaluar la generalización y riesgo de sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3eb34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7ffa4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('OnlineNewsPopularity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bd2fc0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.913725</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799756</td>\n",
       "      <td>0.050047</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>0.050101</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.341246</td>\n",
       "      <td>0.148948</td>\n",
       "      <td>0.043137</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.393365</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217792</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.033351</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.682188</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>0.323333</td>\n",
       "      <td>0.056872</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.404896</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028573</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.494651</td>\n",
       "      <td>0.028905</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.429850</td>\n",
       "      <td>0.100705</td>\n",
       "      <td>0.041431</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.682836</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>3151.157895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.028794</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.885427</td>\n",
       "      <td>0.513502</td>\n",
       "      <td>0.281003</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.411127</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   timedelta   n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words   n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs   num_videos   average_token_length   num_keywords   data_channel_is_lifestyle   data_channel_is_entertainment   data_channel_is_bus   data_channel_is_socmed   data_channel_is_tech   data_channel_is_world   kw_min_min   kw_max_min   kw_avg_min   kw_min_max   kw_max_max   kw_avg_max   kw_min_avg   kw_max_avg   kw_avg_avg   self_reference_min_shares   self_reference_max_shares   self_reference_avg_sharess   weekday_is_monday   weekday_is_tuesday   weekday_is_wednesday   weekday_is_thursday   weekday_is_friday   weekday_is_saturday   weekday_is_sunday   is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04   global_subjectivity   global_sentiment_polarity   global_rate_positive_words   global_rate_negative_words   rate_positive_words   rate_negative_words  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0             12.0              219.0          0.663594                1.0                   0.815385         4.0              2.0        1.0          0.0               4.680365            5.0                         0.0                             1.0                   0.0                      0.0                    0.0                     0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0                       496.0                       496.0                   496.000000                 1.0                  0.0                    0.0                   0.0                 0.0                   0.0                 0.0          0.0  0.500331  0.378279  0.040005  0.041263  0.040123              0.521617                    0.092562                     0.045662                     0.013699              0.769231              0.230769   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...       731.0              9.0              255.0          0.604743                1.0                   0.791946         3.0              1.0        1.0          0.0               4.913725            4.0                         0.0                             0.0                   1.0                      0.0                    0.0                     0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0                         0.0                         0.0                     0.000000                 1.0                  0.0                    0.0                   0.0                 0.0                   0.0                 0.0          0.0  0.799756  0.050047  0.050096  0.050101  0.050001              0.341246                    0.148948                     0.043137                     0.015686              0.733333              0.266667   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...       731.0              9.0              211.0          0.575130                1.0                   0.663866         3.0              1.0        1.0          0.0               4.393365            6.0                         0.0                             0.0                   1.0                      0.0                    0.0                     0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0                       918.0                       918.0                   918.000000                 1.0                  0.0                    0.0                   0.0                 0.0                   0.0                 0.0          0.0  0.217792  0.033334  0.033351  0.033334  0.682188              0.702222                    0.323333                     0.056872                     0.009479              0.857143              0.142857   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...       731.0              9.0              531.0          0.503788                1.0                   0.665635         9.0              0.0        1.0          0.0               4.404896            7.0                         0.0                             1.0                   0.0                      0.0                    0.0                     0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0                         0.0                         0.0                     0.000000                 1.0                  0.0                    0.0                   0.0                 0.0                   0.0                 0.0          0.0  0.028573  0.419300  0.494651  0.028905  0.028572              0.429850                    0.100705                     0.041431                     0.020716              0.666667              0.333333   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/       731.0             13.0             1072.0          0.415646                1.0                   0.540890        19.0             19.0       20.0          0.0               4.682836            7.0                         0.0                             0.0                   0.0                      0.0                    1.0                     0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0          0.0                       545.0                     16000.0                  3151.157895                 1.0                  0.0                    0.0                   0.0                 0.0                   0.0                 0.0          0.0  0.028633  0.028794  0.028575  0.028572  0.885427              0.513502                    0.281003                     0.074627                     0.012127              0.860215              0.139785   \n",
       "\n",
       "    avg_positive_polarity   min_positive_polarity   max_positive_polarity   avg_negative_polarity   min_negative_polarity   max_negative_polarity   title_subjectivity   title_sentiment_polarity   abs_title_subjectivity   abs_title_sentiment_polarity   shares  \n",
       "0                0.378636                0.100000                     0.7               -0.350000                  -0.600               -0.200000             0.500000                  -0.187500                 0.000000                       0.187500      593  \n",
       "1                0.286915                0.033333                     0.7               -0.118750                  -0.125               -0.100000             0.000000                   0.000000                 0.500000                       0.000000      711  \n",
       "2                0.495833                0.100000                     1.0               -0.466667                  -0.800               -0.133333             0.000000                   0.000000                 0.500000                       0.000000     1500  \n",
       "3                0.385965                0.136364                     0.8               -0.369697                  -0.600               -0.166667             0.000000                   0.000000                 0.500000                       0.000000     1200  \n",
       "4                0.411127                0.033333                     1.0               -0.220192                  -0.500               -0.050000             0.454545                   0.136364                 0.045455                       0.136364      505  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c46ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa57edec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.913725</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799756</td>\n",
       "      <td>0.050047</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>0.050101</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.341246</td>\n",
       "      <td>0.148948</td>\n",
       "      <td>0.043137</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.393365</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217792</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.033351</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.682188</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>0.323333</td>\n",
       "      <td>0.056872</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.404896</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028573</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.494651</td>\n",
       "      <td>0.028905</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.429850</td>\n",
       "      <td>0.100705</td>\n",
       "      <td>0.041431</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.682836</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>3151.157895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.028794</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.885427</td>\n",
       "      <td>0.513502</td>\n",
       "      <td>0.281003</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.411127</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  num_videos  average_token_length  num_keywords  data_channel_is_lifestyle  data_channel_is_entertainment  data_channel_is_bus  data_channel_is_socmed  data_channel_is_tech  data_channel_is_world  kw_min_min  kw_max_min  kw_avg_min  kw_min_max  kw_max_max  kw_avg_max  kw_min_avg  kw_max_avg  kw_avg_avg  self_reference_min_shares  self_reference_max_shares  self_reference_avg_sharess  weekday_is_monday  weekday_is_tuesday  weekday_is_wednesday  weekday_is_thursday  weekday_is_friday  weekday_is_saturday  weekday_is_sunday  is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  global_subjectivity  global_sentiment_polarity  global_rate_positive_words  global_rate_negative_words  rate_positive_words  rate_negative_words  avg_positive_polarity  min_positive_polarity  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...      731.0            12.0             219.0         0.663594               1.0                  0.815385        4.0             2.0       1.0         0.0              4.680365           5.0                        0.0                            1.0                  0.0                     0.0                   0.0                    0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                      496.0                      496.0                  496.000000                1.0                 0.0                   0.0                  0.0                0.0                  0.0                0.0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123             0.521617                   0.092562                    0.045662                    0.013699             0.769231             0.230769               0.378636               0.100000   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...      731.0             9.0             255.0         0.604743               1.0                  0.791946        3.0             1.0       1.0         0.0              4.913725           4.0                        0.0                            0.0                  1.0                     0.0                   0.0                    0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                        0.0                        0.0                    0.000000                1.0                 0.0                   0.0                  0.0                0.0                  0.0                0.0         0.0  0.799756  0.050047  0.050096  0.050101  0.050001             0.341246                   0.148948                    0.043137                    0.015686             0.733333             0.266667               0.286915               0.033333   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...      731.0             9.0             211.0         0.575130               1.0                  0.663866        3.0             1.0       1.0         0.0              4.393365           6.0                        0.0                            0.0                  1.0                     0.0                   0.0                    0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                      918.0                      918.0                  918.000000                1.0                 0.0                   0.0                  0.0                0.0                  0.0                0.0         0.0  0.217792  0.033334  0.033351  0.033334  0.682188             0.702222                   0.323333                    0.056872                    0.009479             0.857143             0.142857               0.495833               0.100000   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...      731.0             9.0             531.0         0.503788               1.0                  0.665635        9.0             0.0       1.0         0.0              4.404896           7.0                        0.0                            1.0                  0.0                     0.0                   0.0                    0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                        0.0                        0.0                    0.000000                1.0                 0.0                   0.0                  0.0                0.0                  0.0                0.0         0.0  0.028573  0.419300  0.494651  0.028905  0.028572             0.429850                   0.100705                    0.041431                    0.020716             0.666667             0.333333               0.385965               0.136364   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/      731.0            13.0            1072.0         0.415646               1.0                  0.540890       19.0            19.0      20.0         0.0              4.682836           7.0                        0.0                            0.0                  0.0                     0.0                   1.0                    0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                      545.0                    16000.0                 3151.157895                1.0                 0.0                   0.0                  0.0                0.0                  0.0                0.0         0.0  0.028633  0.028794  0.028575  0.028572  0.885427             0.513502                   0.281003                    0.074627                    0.012127             0.860215             0.139785               0.411127               0.033333   \n",
       "\n",
       "   max_positive_polarity  avg_negative_polarity  min_negative_polarity  max_negative_polarity  title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  abs_title_sentiment_polarity  shares  \n",
       "0                    0.7              -0.350000                 -0.600              -0.200000            0.500000                 -0.187500                0.000000                      0.187500     593  \n",
       "1                    0.7              -0.118750                 -0.125              -0.100000            0.000000                  0.000000                0.500000                      0.000000     711  \n",
       "2                    1.0              -0.466667                 -0.800              -0.133333            0.000000                  0.000000                0.500000                      0.000000    1500  \n",
       "3                    0.8              -0.369697                 -0.600              -0.166667            0.000000                  0.000000                0.500000                      0.000000    1200  \n",
       "4                    1.0              -0.220192                 -0.500              -0.050000            0.454545                  0.136364                0.045455                      0.136364     505  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0c598568",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['url', 'timedelta']\n",
    "df_clean = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a6e5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_binarias = [col for col in df_clean.columns if 'weekday_is' in col or 'data_channel_is' in col]\n",
    "\n",
    "for col in cols_binarias:\n",
    "    df_clean[col] = df_clean[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63c1e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_shares = df_clean['shares'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "670554f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['is_popular'] = (df_clean['shares'] >= median_shares).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "02a24f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['log_shares'] = np.log1p(df_clean['shares'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a808b3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>is_popular</th>\n",
       "      <th>log_shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.398749</td>\n",
       "      <td>546.514731</td>\n",
       "      <td>0.548216</td>\n",
       "      <td>0.996469</td>\n",
       "      <td>0.689175</td>\n",
       "      <td>10.883690</td>\n",
       "      <td>3.293638</td>\n",
       "      <td>4.544143</td>\n",
       "      <td>1.249874</td>\n",
       "      <td>4.548239</td>\n",
       "      <td>7.223767</td>\n",
       "      <td>0.052946</td>\n",
       "      <td>0.178009</td>\n",
       "      <td>0.157855</td>\n",
       "      <td>0.058597</td>\n",
       "      <td>0.185299</td>\n",
       "      <td>0.212567</td>\n",
       "      <td>26.106801</td>\n",
       "      <td>1153.951682</td>\n",
       "      <td>312.366967</td>\n",
       "      <td>13612.354102</td>\n",
       "      <td>752324.066694</td>\n",
       "      <td>259281.938083</td>\n",
       "      <td>1117.146610</td>\n",
       "      <td>5657.211151</td>\n",
       "      <td>3135.858639</td>\n",
       "      <td>3998.755396</td>\n",
       "      <td>10329.212662</td>\n",
       "      <td>6401.697580</td>\n",
       "      <td>0.168020</td>\n",
       "      <td>0.186409</td>\n",
       "      <td>0.187544</td>\n",
       "      <td>0.183306</td>\n",
       "      <td>0.143805</td>\n",
       "      <td>0.061876</td>\n",
       "      <td>0.069039</td>\n",
       "      <td>0.130915</td>\n",
       "      <td>0.184599</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.216321</td>\n",
       "      <td>0.223770</td>\n",
       "      <td>0.234029</td>\n",
       "      <td>0.443370</td>\n",
       "      <td>0.119309</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.682150</td>\n",
       "      <td>0.287934</td>\n",
       "      <td>0.353825</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>-0.259524</td>\n",
       "      <td>-0.521944</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.156064</td>\n",
       "      <td>3395.380184</td>\n",
       "      <td>0.533599</td>\n",
       "      <td>7.475692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.114037</td>\n",
       "      <td>471.107508</td>\n",
       "      <td>3.520708</td>\n",
       "      <td>5.231231</td>\n",
       "      <td>3.264816</td>\n",
       "      <td>11.332017</td>\n",
       "      <td>3.855141</td>\n",
       "      <td>8.309434</td>\n",
       "      <td>4.107855</td>\n",
       "      <td>0.844406</td>\n",
       "      <td>1.909130</td>\n",
       "      <td>0.223929</td>\n",
       "      <td>0.382525</td>\n",
       "      <td>0.364610</td>\n",
       "      <td>0.234871</td>\n",
       "      <td>0.388545</td>\n",
       "      <td>0.409129</td>\n",
       "      <td>69.633215</td>\n",
       "      <td>3857.990877</td>\n",
       "      <td>620.783887</td>\n",
       "      <td>57986.029357</td>\n",
       "      <td>214502.129573</td>\n",
       "      <td>135102.247285</td>\n",
       "      <td>1137.456951</td>\n",
       "      <td>6098.871957</td>\n",
       "      <td>1318.150397</td>\n",
       "      <td>19738.670516</td>\n",
       "      <td>41027.576613</td>\n",
       "      <td>24211.332231</td>\n",
       "      <td>0.373889</td>\n",
       "      <td>0.389441</td>\n",
       "      <td>0.390353</td>\n",
       "      <td>0.386922</td>\n",
       "      <td>0.350896</td>\n",
       "      <td>0.240933</td>\n",
       "      <td>0.253524</td>\n",
       "      <td>0.337312</td>\n",
       "      <td>0.262975</td>\n",
       "      <td>0.219707</td>\n",
       "      <td>0.282145</td>\n",
       "      <td>0.295191</td>\n",
       "      <td>0.289183</td>\n",
       "      <td>0.116685</td>\n",
       "      <td>0.096931</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.190206</td>\n",
       "      <td>0.156156</td>\n",
       "      <td>0.104542</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.226294</td>\n",
       "      <td>11626.950749</td>\n",
       "      <td>0.498876</td>\n",
       "      <td>0.929674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.393750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.470870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625739</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.478404</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>141.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>172846.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3562.101631</td>\n",
       "      <td>2382.448566</td>\n",
       "      <td>639.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>981.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.396167</td>\n",
       "      <td>0.057757</td>\n",
       "      <td>0.028384</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.306244</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328383</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>946.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.853299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>0.539226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.664082</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>235.500000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>244572.222223</td>\n",
       "      <td>1023.635611</td>\n",
       "      <td>4355.688836</td>\n",
       "      <td>2870.074878</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>2200.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.040727</td>\n",
       "      <td>0.453457</td>\n",
       "      <td>0.119117</td>\n",
       "      <td>0.039023</td>\n",
       "      <td>0.015337</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.253333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.244942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754630</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.854839</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>330980.000000</td>\n",
       "      <td>2056.781032</td>\n",
       "      <td>6019.953968</td>\n",
       "      <td>3600.229564</td>\n",
       "      <td>2600.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>5200.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240958</td>\n",
       "      <td>0.150831</td>\n",
       "      <td>0.334218</td>\n",
       "      <td>0.375763</td>\n",
       "      <td>0.399986</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.177832</td>\n",
       "      <td>0.050279</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.411428</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186905</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.937732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>1042.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>8.041534</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>298400.000000</td>\n",
       "      <td>42827.857143</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>3613.039819</td>\n",
       "      <td>298400.000000</td>\n",
       "      <td>43567.659946</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926994</td>\n",
       "      <td>0.925947</td>\n",
       "      <td>0.919999</td>\n",
       "      <td>0.926534</td>\n",
       "      <td>0.927191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727841</td>\n",
       "      <td>0.155488</td>\n",
       "      <td>0.184932</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.645079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  n_non_stop_unique_tokens     num_hrefs  num_self_hrefs      num_imgs    num_videos  average_token_length  num_keywords  data_channel_is_lifestyle  data_channel_is_entertainment  data_channel_is_bus  data_channel_is_socmed  data_channel_is_tech  data_channel_is_world    kw_min_min     kw_max_min    kw_avg_min     kw_min_max     kw_max_max     kw_avg_max    kw_min_avg     kw_max_avg    kw_avg_avg  self_reference_min_shares  self_reference_max_shares  self_reference_avg_sharess  weekday_is_monday  weekday_is_tuesday  weekday_is_wednesday  weekday_is_thursday  weekday_is_friday  weekday_is_saturday  weekday_is_sunday    is_weekend        LDA_00        LDA_01        LDA_02        LDA_03        LDA_04  global_subjectivity  global_sentiment_polarity  global_rate_positive_words  global_rate_negative_words  rate_positive_words  rate_negative_words  avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "count    39644.000000      39644.000000     39644.000000      39644.000000              39644.000000  39644.000000    39644.000000  39644.000000  39644.000000          39644.000000  39644.000000               39644.000000                   39644.000000         39644.000000            39644.000000          39644.000000           39644.000000  39644.000000   39644.000000  39644.000000   39644.000000   39644.000000   39644.000000  39644.000000   39644.000000  39644.000000               39644.000000               39644.000000                39644.000000       39644.000000        39644.000000          39644.000000         39644.000000       39644.000000         39644.000000       39644.000000  39644.000000  39644.000000  39644.000000  39644.000000  39644.000000  39644.000000         39644.000000               39644.000000                39644.000000                39644.000000         39644.000000         39644.000000           39644.000000           39644.000000           39644.000000   \n",
       "mean        10.398749        546.514731         0.548216          0.996469                  0.689175     10.883690        3.293638      4.544143      1.249874              4.548239      7.223767                   0.052946                       0.178009             0.157855                0.058597              0.185299               0.212567     26.106801    1153.951682    312.366967   13612.354102  752324.066694  259281.938083   1117.146610    5657.211151   3135.858639                3998.755396               10329.212662                 6401.697580           0.168020            0.186409              0.187544             0.183306           0.143805             0.061876           0.069039      0.130915      0.184599      0.141256      0.216321      0.223770      0.234029             0.443370                   0.119309                    0.039625                    0.016612             0.682150             0.287934               0.353825               0.095446               0.756728   \n",
       "std          2.114037        471.107508         3.520708          5.231231                  3.264816     11.332017        3.855141      8.309434      4.107855              0.844406      1.909130                   0.223929                       0.382525             0.364610                0.234871              0.388545               0.409129     69.633215    3857.990877    620.783887   57986.029357  214502.129573  135102.247285   1137.456951    6098.871957   1318.150397               19738.670516               41027.576613                24211.332231           0.373889            0.389441              0.390353             0.386922           0.350896             0.240933           0.253524      0.337312      0.262975      0.219707      0.282145      0.295191      0.289183             0.116685                   0.096931                    0.017429                    0.010828             0.190206             0.156156               0.104542               0.071315               0.247786   \n",
       "min          2.000000          0.000000         0.000000          0.000000                  0.000000      0.000000        0.000000      0.000000      0.000000              0.000000      1.000000                   0.000000                       0.000000             0.000000                0.000000              0.000000               0.000000     -1.000000       0.000000     -1.000000       0.000000       0.000000       0.000000     -1.000000       0.000000      0.000000                   0.000000                   0.000000                    0.000000           0.000000            0.000000              0.000000             0.000000           0.000000             0.000000           0.000000      0.000000      0.000000      0.000000      0.000000      0.000000      0.000000             0.000000                  -0.393750                    0.000000                    0.000000             0.000000             0.000000               0.000000               0.000000               0.000000   \n",
       "25%          9.000000        246.000000         0.470870          1.000000                  0.625739      4.000000        1.000000      1.000000      0.000000              4.478404      6.000000                   0.000000                       0.000000             0.000000                0.000000              0.000000               0.000000     -1.000000     445.000000    141.750000       0.000000  843300.000000  172846.875000      0.000000    3562.101631   2382.448566                 639.000000                1100.000000                  981.187500           0.000000            0.000000              0.000000             0.000000           0.000000             0.000000           0.000000      0.000000      0.025051      0.025012      0.028571      0.028571      0.028574             0.396167                   0.057757                    0.028384                    0.009615             0.600000             0.185185               0.306244               0.050000               0.600000   \n",
       "50%         10.000000        409.000000         0.539226          1.000000                  0.690476      8.000000        3.000000      1.000000      0.000000              4.664082      7.000000                   0.000000                       0.000000             0.000000                0.000000              0.000000               0.000000     -1.000000     660.000000    235.500000    1400.000000  843300.000000  244572.222223   1023.635611    4355.688836   2870.074878                1200.000000                2800.000000                 2200.000000           0.000000            0.000000              0.000000             0.000000           0.000000             0.000000           0.000000      0.000000      0.033387      0.033345      0.040004      0.040001      0.040727             0.453457                   0.119117                    0.039023                    0.015337             0.710526             0.280000               0.358755               0.100000               0.800000   \n",
       "75%         12.000000        716.000000         0.608696          1.000000                  0.754630     14.000000        4.000000      4.000000      1.000000              4.854839      9.000000                   0.000000                       0.000000             0.000000                0.000000              0.000000               0.000000      4.000000    1000.000000    357.000000    7900.000000  843300.000000  330980.000000   2056.781032    6019.953968   3600.229564                2600.000000                8000.000000                 5200.000000           0.000000            0.000000              0.000000             0.000000           0.000000             0.000000           0.000000      0.000000      0.240958      0.150831      0.334218      0.375763      0.399986             0.508333                   0.177832                    0.050279                    0.021739             0.800000             0.384615               0.411428               0.100000               1.000000   \n",
       "max         23.000000       8474.000000       701.000000       1042.000000                650.000000    304.000000      116.000000    128.000000     91.000000              8.041534     10.000000                   1.000000                       1.000000             1.000000                1.000000              1.000000               1.000000    377.000000  298400.000000  42827.857143  843300.000000  843300.000000  843300.000000   3613.039819  298400.000000  43567.659946              843300.000000              843300.000000               843300.000000           1.000000            1.000000              1.000000             1.000000           1.000000             1.000000           1.000000      1.000000      0.926994      0.925947      0.919999      0.926534      0.927191             1.000000                   0.727841                    0.155488                    0.184932             1.000000             1.000000               1.000000               1.000000               1.000000   \n",
       "\n",
       "       avg_negative_polarity  min_negative_polarity  max_negative_polarity  title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  abs_title_sentiment_polarity         shares    is_popular    log_shares  \n",
       "count           39644.000000           39644.000000           39644.000000        39644.000000              39644.000000            39644.000000                  39644.000000   39644.000000  39644.000000  39644.000000  \n",
       "mean               -0.259524              -0.521944              -0.107500            0.282353                  0.071425                0.341843                      0.156064    3395.380184      0.533599      7.475692  \n",
       "std                 0.127726               0.290290               0.095373            0.324247                  0.265450                0.188791                      0.226294   11626.950749      0.498876      0.929674  \n",
       "min                -1.000000              -1.000000              -1.000000            0.000000                 -1.000000                0.000000                      0.000000       1.000000      0.000000      0.693147  \n",
       "25%                -0.328383              -0.700000              -0.125000            0.000000                  0.000000                0.166667                      0.000000     946.000000      0.000000      6.853299  \n",
       "50%                -0.253333              -0.500000              -0.100000            0.150000                  0.000000                0.500000                      0.000000    1400.000000      1.000000      7.244942  \n",
       "75%                -0.186905              -0.300000              -0.050000            0.500000                  0.150000                0.500000                      0.250000    2800.000000      1.000000      7.937732  \n",
       "max                 0.000000               0.000000               0.000000            1.000000                  1.000000                0.500000                      1.000000  843300.000000      1.000000     13.645079  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f4d8b32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39644 entries, 0 to 39643\n",
      "Data columns (total 61 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   n_tokens_title                 39644 non-null  float64\n",
      " 1   n_tokens_content               39644 non-null  float64\n",
      " 2   n_unique_tokens                39644 non-null  float64\n",
      " 3   n_non_stop_words               39644 non-null  float64\n",
      " 4   n_non_stop_unique_tokens       39644 non-null  float64\n",
      " 5   num_hrefs                      39644 non-null  float64\n",
      " 6   num_self_hrefs                 39644 non-null  float64\n",
      " 7   num_imgs                       39644 non-null  float64\n",
      " 8   num_videos                     39644 non-null  float64\n",
      " 9   average_token_length           39644 non-null  float64\n",
      " 10  num_keywords                   39644 non-null  float64\n",
      " 11  data_channel_is_lifestyle      39644 non-null  int64  \n",
      " 12  data_channel_is_entertainment  39644 non-null  int64  \n",
      " 13  data_channel_is_bus            39644 non-null  int64  \n",
      " 14  data_channel_is_socmed         39644 non-null  int64  \n",
      " 15  data_channel_is_tech           39644 non-null  int64  \n",
      " 16  data_channel_is_world          39644 non-null  int64  \n",
      " 17  kw_min_min                     39644 non-null  float64\n",
      " 18  kw_max_min                     39644 non-null  float64\n",
      " 19  kw_avg_min                     39644 non-null  float64\n",
      " 20  kw_min_max                     39644 non-null  float64\n",
      " 21  kw_max_max                     39644 non-null  float64\n",
      " 22  kw_avg_max                     39644 non-null  float64\n",
      " 23  kw_min_avg                     39644 non-null  float64\n",
      " 24  kw_max_avg                     39644 non-null  float64\n",
      " 25  kw_avg_avg                     39644 non-null  float64\n",
      " 26  self_reference_min_shares      39644 non-null  float64\n",
      " 27  self_reference_max_shares      39644 non-null  float64\n",
      " 28  self_reference_avg_sharess     39644 non-null  float64\n",
      " 29  weekday_is_monday              39644 non-null  int64  \n",
      " 30  weekday_is_tuesday             39644 non-null  int64  \n",
      " 31  weekday_is_wednesday           39644 non-null  int64  \n",
      " 32  weekday_is_thursday            39644 non-null  int64  \n",
      " 33  weekday_is_friday              39644 non-null  int64  \n",
      " 34  weekday_is_saturday            39644 non-null  int64  \n",
      " 35  weekday_is_sunday              39644 non-null  int64  \n",
      " 36  is_weekend                     39644 non-null  float64\n",
      " 37  LDA_00                         39644 non-null  float64\n",
      " 38  LDA_01                         39644 non-null  float64\n",
      " 39  LDA_02                         39644 non-null  float64\n",
      " 40  LDA_03                         39644 non-null  float64\n",
      " 41  LDA_04                         39644 non-null  float64\n",
      " 42  global_subjectivity            39644 non-null  float64\n",
      " 43  global_sentiment_polarity      39644 non-null  float64\n",
      " 44  global_rate_positive_words     39644 non-null  float64\n",
      " 45  global_rate_negative_words     39644 non-null  float64\n",
      " 46  rate_positive_words            39644 non-null  float64\n",
      " 47  rate_negative_words            39644 non-null  float64\n",
      " 48  avg_positive_polarity          39644 non-null  float64\n",
      " 49  min_positive_polarity          39644 non-null  float64\n",
      " 50  max_positive_polarity          39644 non-null  float64\n",
      " 51  avg_negative_polarity          39644 non-null  float64\n",
      " 52  min_negative_polarity          39644 non-null  float64\n",
      " 53  max_negative_polarity          39644 non-null  float64\n",
      " 54  title_subjectivity             39644 non-null  float64\n",
      " 55  title_sentiment_polarity       39644 non-null  float64\n",
      " 56  abs_title_subjectivity         39644 non-null  float64\n",
      " 57  abs_title_sentiment_polarity   39644 non-null  float64\n",
      " 58  shares                         39644 non-null  int64  \n",
      " 59  is_popular                     39644 non-null  int64  \n",
      " 60  log_shares                     39644 non-null  float64\n",
      "dtypes: float64(46), int64(15)\n",
      "memory usage: 18.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b690d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = ['shares', 'log_shares', 'is_popular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "570b9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_clean.drop(columns=cols_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1e13037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = df_clean['log_shares']      #esta es para regresion mateo\n",
    "y_class = df_clean['is_popular']    #esta es para clasificacion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ae3db11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  num_videos  average_token_length  num_keywords  data_channel_is_lifestyle  data_channel_is_entertainment  data_channel_is_bus  data_channel_is_socmed  data_channel_is_tech  data_channel_is_world  kw_min_min  kw_max_min  kw_avg_min  kw_min_max  kw_max_max  kw_avg_max  kw_min_avg  kw_max_avg  kw_avg_avg  self_reference_min_shares  self_reference_max_shares  self_reference_avg_sharess  weekday_is_monday  weekday_is_tuesday  weekday_is_wednesday  weekday_is_thursday  weekday_is_friday  weekday_is_saturday  weekday_is_sunday  is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  global_subjectivity  global_sentiment_polarity  global_rate_positive_words  global_rate_negative_words  rate_positive_words  rate_negative_words  avg_positive_polarity  min_positive_polarity  max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
      "0            12.0             219.0         0.663594               1.0                  0.815385        4.0             2.0       1.0         0.0              4.680365           5.0                          0                              1                    0                       0                     0                      0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                      496.0                      496.0                  496.000000                  1                   0                     0                    0                  0                    0                  0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123             0.521617                   0.092562                    0.045662                    0.013699             0.769231             0.230769               0.378636               0.100000                    0.7              -0.350000                 -0.600   \n",
      "1             9.0             255.0         0.604743               1.0                  0.791946        3.0             1.0       1.0         0.0              4.913725           4.0                          0                              0                    1                       0                     0                      0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                        0.0                        0.0                    0.000000                  1                   0                     0                    0                  0                    0                  0         0.0  0.799756  0.050047  0.050096  0.050101  0.050001             0.341246                   0.148948                    0.043137                    0.015686             0.733333             0.266667               0.286915               0.033333                    0.7              -0.118750                 -0.125   \n",
      "2             9.0             211.0         0.575130               1.0                  0.663866        3.0             1.0       1.0         0.0              4.393365           6.0                          0                              0                    1                       0                     0                      0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                      918.0                      918.0                  918.000000                  1                   0                     0                    0                  0                    0                  0         0.0  0.217792  0.033334  0.033351  0.033334  0.682188             0.702222                   0.323333                    0.056872                    0.009479             0.857143             0.142857               0.495833               0.100000                    1.0              -0.466667                 -0.800   \n",
      "3             9.0             531.0         0.503788               1.0                  0.665635        9.0             0.0       1.0         0.0              4.404896           7.0                          0                              1                    0                       0                     0                      0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                        0.0                        0.0                    0.000000                  1                   0                     0                    0                  0                    0                  0         0.0  0.028573  0.419300  0.494651  0.028905  0.028572             0.429850                   0.100705                    0.041431                    0.020716             0.666667             0.333333               0.385965               0.136364                    0.8              -0.369697                 -0.600   \n",
      "4            13.0            1072.0         0.415646               1.0                  0.540890       19.0            19.0      20.0         0.0              4.682836           7.0                          0                              0                    0                       0                     1                      0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                      545.0                    16000.0                 3151.157895                  1                   0                     0                    0                  0                    0                  0         0.0  0.028633  0.028794  0.028575  0.028572  0.885427             0.513502                   0.281003                    0.074627                    0.012127             0.860215             0.139785               0.411127               0.033333                    1.0              -0.220192                 -0.500   \n",
      "\n",
      "   max_negative_polarity  title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  abs_title_sentiment_polarity  shares  is_popular  log_shares  \n",
      "0              -0.200000            0.500000                 -0.187500                0.000000                      0.187500     593           0    6.386879  \n",
      "1              -0.100000            0.000000                  0.000000                0.500000                      0.000000     711           0    6.568078  \n",
      "2              -0.133333            0.000000                  0.000000                0.500000                      0.000000    1500           1    7.313887  \n",
      "3              -0.166667            0.000000                  0.000000                0.500000                      0.000000    1200           0    7.090910  \n",
      "4              -0.050000            0.454545                  0.136364                0.045455                      0.136364     505           0    6.226537  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 1000) \n",
    "\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7fe02",
   "metadata": {},
   "source": [
    "# Modelos Propuestos. (Se integra el preprocesamiento al pipeline directamente)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0528e6",
   "metadata": {},
   "source": [
    "## Clasificacion: Variable a clasificar 'Is Popular' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a86179",
   "metadata": {},
   "source": [
    "Clasificaremos sin tener la información de los shares, es decir, a partir de toda la información sin tener los shares veremos si se hace popular o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d136f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas= df_clean.drop(columns=[\"shares\", \"log_shares\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ae5af312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>is_popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.913725</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799756</td>\n",
       "      <td>0.050047</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>0.050101</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.341246</td>\n",
       "      <td>0.148948</td>\n",
       "      <td>0.043137</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.393365</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217792</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.033351</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.682188</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>0.323333</td>\n",
       "      <td>0.056872</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.404896</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028573</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.494651</td>\n",
       "      <td>0.028905</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.429850</td>\n",
       "      <td>0.100705</td>\n",
       "      <td>0.041431</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.682836</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>3151.157895</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.028794</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.885427</td>\n",
       "      <td>0.513502</td>\n",
       "      <td>0.281003</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.411127</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  num_videos  average_token_length  num_keywords  data_channel_is_lifestyle  data_channel_is_entertainment  data_channel_is_bus  data_channel_is_socmed  data_channel_is_tech  data_channel_is_world  kw_min_min  kw_max_min  kw_avg_min  kw_min_max  kw_max_max  kw_avg_max  kw_min_avg  kw_max_avg  kw_avg_avg  self_reference_min_shares  self_reference_max_shares  self_reference_avg_sharess  weekday_is_monday  weekday_is_tuesday  weekday_is_wednesday  weekday_is_thursday  weekday_is_friday  weekday_is_saturday  weekday_is_sunday  is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  global_subjectivity  global_sentiment_polarity  global_rate_positive_words  global_rate_negative_words  rate_positive_words  rate_negative_words  avg_positive_polarity  min_positive_polarity  max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "0            12.0             219.0         0.663594               1.0                  0.815385        4.0             2.0       1.0         0.0              4.680365           5.0                          0                              1                    0                       0                     0                      0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                      496.0                      496.0                  496.000000                  1                   0                     0                    0                  0                    0                  0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123             0.521617                   0.092562                    0.045662                    0.013699             0.769231             0.230769               0.378636               0.100000                    0.7              -0.350000                 -0.600   \n",
       "1             9.0             255.0         0.604743               1.0                  0.791946        3.0             1.0       1.0         0.0              4.913725           4.0                          0                              0                    1                       0                     0                      0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                        0.0                        0.0                    0.000000                  1                   0                     0                    0                  0                    0                  0         0.0  0.799756  0.050047  0.050096  0.050101  0.050001             0.341246                   0.148948                    0.043137                    0.015686             0.733333             0.266667               0.286915               0.033333                    0.7              -0.118750                 -0.125   \n",
       "2             9.0             211.0         0.575130               1.0                  0.663866        3.0             1.0       1.0         0.0              4.393365           6.0                          0                              0                    1                       0                     0                      0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                      918.0                      918.0                  918.000000                  1                   0                     0                    0                  0                    0                  0         0.0  0.217792  0.033334  0.033351  0.033334  0.682188             0.702222                   0.323333                    0.056872                    0.009479             0.857143             0.142857               0.495833               0.100000                    1.0              -0.466667                 -0.800   \n",
       "3             9.0             531.0         0.503788               1.0                  0.665635        9.0             0.0       1.0         0.0              4.404896           7.0                          0                              1                    0                       0                     0                      0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                        0.0                        0.0                    0.000000                  1                   0                     0                    0                  0                    0                  0         0.0  0.028573  0.419300  0.494651  0.028905  0.028572             0.429850                   0.100705                    0.041431                    0.020716             0.666667             0.333333               0.385965               0.136364                    0.8              -0.369697                 -0.600   \n",
       "4            13.0            1072.0         0.415646               1.0                  0.540890       19.0            19.0      20.0         0.0              4.682836           7.0                          0                              0                    0                       0                     1                      0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0         0.0                      545.0                    16000.0                 3151.157895                  1                   0                     0                    0                  0                    0                  0         0.0  0.028633  0.028794  0.028575  0.028572  0.885427             0.513502                   0.281003                    0.074627                    0.012127             0.860215             0.139785               0.411127               0.033333                    1.0              -0.220192                 -0.500   \n",
       "\n",
       "   max_negative_polarity  title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  abs_title_sentiment_polarity  is_popular  \n",
       "0              -0.200000            0.500000                 -0.187500                0.000000                      0.187500           0  \n",
       "1              -0.100000            0.000000                  0.000000                0.500000                      0.000000           0  \n",
       "2              -0.133333            0.000000                  0.000000                0.500000                      0.000000           1  \n",
       "3              -0.166667            0.000000                  0.000000                0.500000                      0.000000           0  \n",
       "4              -0.050000            0.454545                  0.136364                0.045455                      0.136364           0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9ead1bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 16:36:35,363] A new study created in memory with name: no-name-a14fc568-9ca6-4290-a977-6339448a3ee6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando optimización...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 16:36:56,748] Trial 0 finished with value: 0.7355476958636649 and parameters: {'n_estimators': 746, 'learning_rate': 0.033862500461139096, 'max_depth': 7, 'subsample': 0.7002754093534714}. Best is trial 0 with value: 0.7355476958636649.\n",
      "[I 2025-12-05 16:37:14,169] Trial 1 finished with value: 0.7220883845891123 and parameters: {'n_estimators': 452, 'learning_rate': 0.17200469488292972, 'max_depth': 9, 'subsample': 0.9909626630820184}. Best is trial 0 with value: 0.7355476958636649.\n",
      "[I 2025-12-05 16:37:20,650] Trial 2 finished with value: 0.7238427980686378 and parameters: {'n_estimators': 340, 'learning_rate': 0.007926813987947028, 'max_depth': 4, 'subsample': 0.8917498138230726}. Best is trial 0 with value: 0.7355476958636649.\n",
      "[I 2025-12-05 16:37:38,490] Trial 3 finished with value: 0.7197095134255035 and parameters: {'n_estimators': 584, 'learning_rate': 0.11502516705628378, 'max_depth': 8, 'subsample': 0.7078023655477784}. Best is trial 0 with value: 0.7355476958636649.\n",
      "[I 2025-12-05 16:37:46,685] Trial 4 finished with value: 0.7326686665138572 and parameters: {'n_estimators': 107, 'learning_rate': 0.016949471900036546, 'max_depth': 10, 'subsample': 0.6942359132567172}. Best is trial 0 with value: 0.7355476958636649.\n",
      "[I 2025-12-05 16:37:51,255] Trial 5 finished with value: 0.7314156183068177 and parameters: {'n_estimators': 184, 'learning_rate': 0.01022750803044076, 'max_depth': 7, 'subsample': 0.6472240574118252}. Best is trial 0 with value: 0.7355476958636649.\n",
      "[I 2025-12-05 16:37:56,584] Trial 6 finished with value: 0.7340759888529427 and parameters: {'n_estimators': 790, 'learning_rate': 0.11697380720714537, 'max_depth': 3, 'subsample': 0.6436537021672186}. Best is trial 0 with value: 0.7355476958636649.\n",
      "[I 2025-12-05 16:37:59,555] Trial 7 finished with value: 0.7288469513672658 and parameters: {'n_estimators': 360, 'learning_rate': 0.01692505168551837, 'max_depth': 3, 'subsample': 0.7667410264055795}. Best is trial 0 with value: 0.7355476958636649.\n",
      "[I 2025-12-05 16:38:24,332] Trial 8 finished with value: 0.7351032226906474 and parameters: {'n_estimators': 332, 'learning_rate': 0.008383843864689459, 'max_depth': 10, 'subsample': 0.9342168635865387}. Best is trial 0 with value: 0.7355476958636649.\n",
      "[I 2025-12-05 16:38:26,794] Trial 9 finished with value: 0.7363211559482792 and parameters: {'n_estimators': 315, 'learning_rate': 0.04996418176359387, 'max_depth': 3, 'subsample': 0.7945438257024239}. Best is trial 9 with value: 0.7363211559482792.\n",
      "[I 2025-12-05 16:38:32,399] Trial 10 finished with value: 0.7393188995395932 and parameters: {'n_estimators': 531, 'learning_rate': 0.05001943893078428, 'max_depth': 5, 'subsample': 0.8506070028515799}. Best is trial 10 with value: 0.7393188995395932.\n",
      "[I 2025-12-05 16:38:38,180] Trial 11 finished with value: 0.7391323033303469 and parameters: {'n_estimators': 550, 'learning_rate': 0.0530970764635945, 'max_depth': 5, 'subsample': 0.8298731851478147}. Best is trial 10 with value: 0.7393188995395932.\n",
      "[I 2025-12-05 16:38:44,308] Trial 12 finished with value: 0.7388486342135044 and parameters: {'n_estimators': 585, 'learning_rate': 0.05434743978100394, 'max_depth': 5, 'subsample': 0.8701955410676507}. Best is trial 10 with value: 0.7393188995395932.\n",
      "[I 2025-12-05 16:38:50,267] Trial 13 finished with value: 0.70419718990739 and parameters: {'n_estimators': 575, 'learning_rate': 0.29580385760379174, 'max_depth': 5, 'subsample': 0.8581092693583862}. Best is trial 10 with value: 0.7393188995395932.\n",
      "[I 2025-12-05 16:38:55,488] Trial 14 finished with value: 0.7409202747690189 and parameters: {'n_estimators': 487, 'learning_rate': 0.032852784434223965, 'max_depth': 5, 'subsample': 0.8289747542166606}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:39:02,289] Trial 15 finished with value: 0.7408563790418712 and parameters: {'n_estimators': 465, 'learning_rate': 0.030738329473346055, 'max_depth': 6, 'subsample': 0.9285822687950824}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:39:11,838] Trial 16 finished with value: 0.7406956997054402 and parameters: {'n_estimators': 684, 'learning_rate': 0.024967576351374046, 'max_depth': 6, 'subsample': 0.9315612804678929}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:39:18,379] Trial 17 finished with value: 0.7386912840363236 and parameters: {'n_estimators': 446, 'learning_rate': 0.023975494620225645, 'max_depth': 6, 'subsample': 0.9965615593127282}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:39:29,981] Trial 18 finished with value: 0.7266035491218831 and parameters: {'n_estimators': 442, 'learning_rate': 0.09460772351100143, 'max_depth': 8, 'subsample': 0.9309131519729952}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:39:32,819] Trial 19 finished with value: 0.7151018829853767 and parameters: {'n_estimators': 270, 'learning_rate': 0.005504074768846827, 'max_depth': 4, 'subsample': 0.7647232618472724}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:39:40,417] Trial 20 finished with value: 0.7396213733927953 and parameters: {'n_estimators': 497, 'learning_rate': 0.01523034392067736, 'max_depth': 6, 'subsample': 0.8988728937103274}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:39:49,615] Trial 21 finished with value: 0.7397455705722606 and parameters: {'n_estimators': 658, 'learning_rate': 0.030769946405364366, 'max_depth': 6, 'subsample': 0.9490973508512461}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:39:54,900] Trial 22 finished with value: 0.7394038821897794 and parameters: {'n_estimators': 653, 'learning_rate': 0.022411040387863675, 'max_depth': 4, 'subsample': 0.9059536627080659}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:40:07,316] Trial 23 finished with value: 0.7304530950599248 and parameters: {'n_estimators': 670, 'learning_rate': 0.07380580052912417, 'max_depth': 7, 'subsample': 0.9601685918329557}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:40:15,252] Trial 24 finished with value: 0.7401134826755387 and parameters: {'n_estimators': 406, 'learning_rate': 0.02663090144631171, 'max_depth': 6, 'subsample': 0.8215277837214255}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:40:46,802] Trial 25 finished with value: 0.735213532874178 and parameters: {'n_estimators': 719, 'learning_rate': 0.03855612329899737, 'max_depth': 8, 'subsample': 0.9722439892879031}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:40:56,254] Trial 26 finished with value: 0.7367192493351411 and parameters: {'n_estimators': 492, 'learning_rate': 0.012634988788275764, 'max_depth': 5, 'subsample': 0.9141464912364002}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:41:01,998] Trial 27 finished with value: 0.7387049912535459 and parameters: {'n_estimators': 239, 'learning_rate': 0.020564902266178284, 'max_depth': 7, 'subsample': 0.7807231917273091}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:41:10,475] Trial 28 finished with value: 0.7297913807050949 and parameters: {'n_estimators': 616, 'learning_rate': 0.07515498845449423, 'max_depth': 6, 'subsample': 0.7438391228225936}. Best is trial 14 with value: 0.7409202747690189.\n",
      "[I 2025-12-05 16:41:24,084] Trial 29 finished with value: 0.738295499286807 and parameters: {'n_estimators': 716, 'learning_rate': 0.03459538852007635, 'max_depth': 7, 'subsample': 0.8749126677016806}. Best is trial 14 with value: 0.7409202747690189.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor ROC AUC: 0.7409\n",
      "Desviación Estándar ROC AUC: 0.0032\n",
      "Accuracy del mejor modelo: 0.6782\n",
      "Desviación Estándar Accuracy: 0.0040\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "\n",
    "target_column = 'is_popular'\n",
    "\n",
    "X = df_clas.drop(columns=[target_column])\n",
    "y = df_clas[target_column]\n",
    "\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, num_cols)     \n",
    "])\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        **param_grid, \n",
    "        random_state=42, \n",
    "        n_jobs=-1, \n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "\n",
    "    cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "   \n",
    "    cv_results = cross_validate(\n",
    "        pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv=cv_inner, \n",
    "        scoring=['accuracy', 'roc_auc'],\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    mean_auc = cv_results['test_roc_auc'].mean()\n",
    "    mean_accuracy = cv_results['test_accuracy'].mean()\n",
    "    \n",
    "    std_auc = cv_results['test_roc_auc'].std()\n",
    "    std_accuracy = cv_results['test_accuracy'].std()\n",
    "\n",
    "    trial.set_user_attr(\"accuracy\", mean_accuracy)\n",
    "    trial.set_user_attr(\"std_auc\", std_auc)       \n",
    "    trial.set_user_attr(\"std_accuracy\", std_accuracy)\n",
    "    \n",
    "    return mean_auc\n",
    "\n",
    "\n",
    "print(\"Iniciando optimización...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30) \n",
    "\n",
    "print(f\"Mejor ROC AUC: {study.best_value:.4f}\")\n",
    "print(f\"Desviación Estándar ROC AUC: {study.best_trial.user_attrs['std_auc']:.4f}\")\n",
    "\n",
    "print(f\"Accuracy del mejor modelo: {study.best_trial.user_attrs['accuracy']:.4f}\")\n",
    "print(f\"Desviación Estándar Accuracy: {study.best_trial.user_attrs['std_accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f393ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'n_estimators': 487, 'learning_rate': 0.032852784434223965, 'max_depth': 5, 'subsample': 0.8289747542166606}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mejores parámetros: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dee483",
   "metadata": {},
   "source": [
    "\n",
    "### Resultado XGBoost de clasificación:\n",
    "- Mejor ROC AUC: 0.7409\n",
    "- Desviación Estándar ROC AUC: 0.0032\n",
    "- Mejores parámetros: 'n_estimators': 487, 'learning_rate': 0.032852784434223965, 'max_depth': 5, 'subsample': 0.8289747542166606\n",
    "- Accuracy del mejor modelo:  0.6782\n",
    "- Desviación Estándar Accuracy: 0.0040\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "619e1632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 16:42:18,866] A new study created in memory with name: no-name-744a508c-983e-4c77-bc89-1ae9184c6f6a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando optimización...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 16:42:30,268] Trial 0 finished with value: 0.7322084874422748 and parameters: {'n_estimators': 241, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.7322084874422748.\n",
      "[I 2025-12-05 16:42:46,513] Trial 1 finished with value: 0.7215758927906059 and parameters: {'n_estimators': 630, 'max_depth': 7, 'min_samples_split': 11, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.7322084874422748.\n",
      "[I 2025-12-05 16:43:02,121] Trial 2 finished with value: 0.7317790949750511 and parameters: {'n_estimators': 271, 'max_depth': 31, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.7322084874422748.\n",
      "[I 2025-12-05 16:43:20,582] Trial 3 finished with value: 0.7270553873158068 and parameters: {'n_estimators': 572, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.7322084874422748.\n",
      "[I 2025-12-05 16:43:30,223] Trial 4 finished with value: 0.7302590618043616 and parameters: {'n_estimators': 242, 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.7322084874422748.\n",
      "[I 2025-12-05 16:43:43,201] Trial 5 finished with value: 0.7290508582158477 and parameters: {'n_estimators': 358, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.7322084874422748.\n",
      "[I 2025-12-05 16:44:01,205] Trial 6 finished with value: 0.7332134885260224 and parameters: {'n_estimators': 392, 'max_depth': 40, 'min_samples_split': 9, 'min_samples_leaf': 10}. Best is trial 6 with value: 0.7332134885260224.\n",
      "[I 2025-12-05 16:44:23,823] Trial 7 finished with value: 0.72712067259291 and parameters: {'n_estimators': 702, 'max_depth': 9, 'min_samples_split': 13, 'min_samples_leaf': 8}. Best is trial 6 with value: 0.7332134885260224.\n",
      "[I 2025-12-05 16:45:00,060] Trial 8 finished with value: 0.7338719819946705 and parameters: {'n_estimators': 784, 'max_depth': 19, 'min_samples_split': 17, 'min_samples_leaf': 8}. Best is trial 8 with value: 0.7338719819946705.\n",
      "[I 2025-12-05 16:45:37,591] Trial 9 finished with value: 0.7336123871336745 and parameters: {'n_estimators': 707, 'max_depth': 31, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 8 with value: 0.7338719819946705.\n",
      "[I 2025-12-05 16:46:15,188] Trial 10 finished with value: 0.7340750833686231 and parameters: {'n_estimators': 795, 'max_depth': 22, 'min_samples_split': 19, 'min_samples_leaf': 5}. Best is trial 10 with value: 0.7340750833686231.\n",
      "[I 2025-12-05 16:46:52,792] Trial 11 finished with value: 0.7338621472088566 and parameters: {'n_estimators': 800, 'max_depth': 21, 'min_samples_split': 20, 'min_samples_leaf': 5}. Best is trial 10 with value: 0.7340750833686231.\n",
      "[I 2025-12-05 16:47:17,476] Trial 12 finished with value: 0.7338936721276885 and parameters: {'n_estimators': 511, 'max_depth': 22, 'min_samples_split': 19, 'min_samples_leaf': 5}. Best is trial 10 with value: 0.7340750833686231.\n",
      "[I 2025-12-05 16:47:22,973] Trial 13 finished with value: 0.7303141178897069 and parameters: {'n_estimators': 100, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 10 with value: 0.7340750833686231.\n",
      "[I 2025-12-05 16:47:48,515] Trial 14 finished with value: 0.7341967324480485 and parameters: {'n_estimators': 518, 'max_depth': 40, 'min_samples_split': 20, 'min_samples_leaf': 4}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:48:12,166] Trial 15 finished with value: 0.7335512277224159 and parameters: {'n_estimators': 474, 'max_depth': 50, 'min_samples_split': 16, 'min_samples_leaf': 4}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:48:42,898] Trial 16 finished with value: 0.7336079452619886 and parameters: {'n_estimators': 633, 'max_depth': 39, 'min_samples_split': 16, 'min_samples_leaf': 6}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:49:09,922] Trial 17 finished with value: 0.7326929372217339 and parameters: {'n_estimators': 512, 'max_depth': 39, 'min_samples_split': 18, 'min_samples_leaf': 1}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:49:30,612] Trial 18 finished with value: 0.7335369324832696 and parameters: {'n_estimators': 415, 'max_depth': 47, 'min_samples_split': 14, 'min_samples_leaf': 4}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:50:03,910] Trial 19 finished with value: 0.7339065120936357 and parameters: {'n_estimators': 705, 'max_depth': 33, 'min_samples_split': 20, 'min_samples_leaf': 6}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:50:33,594] Trial 20 finished with value: 0.7338215630711223 and parameters: {'n_estimators': 594, 'max_depth': 43, 'min_samples_split': 14, 'min_samples_leaf': 4}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:51:08,380] Trial 21 finished with value: 0.7338804054668253 and parameters: {'n_estimators': 715, 'max_depth': 34, 'min_samples_split': 20, 'min_samples_leaf': 6}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:51:43,405] Trial 22 finished with value: 0.733775035964453 and parameters: {'n_estimators': 727, 'max_depth': 35, 'min_samples_split': 18, 'min_samples_leaf': 7}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:52:14,325] Trial 23 finished with value: 0.733996479710885 and parameters: {'n_estimators': 660, 'max_depth': 26, 'min_samples_split': 20, 'min_samples_leaf': 6}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:52:45,934] Trial 24 finished with value: 0.7336825585406463 and parameters: {'n_estimators': 642, 'max_depth': 26, 'min_samples_split': 16, 'min_samples_leaf': 4}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:53:11,898] Trial 25 finished with value: 0.7328506034546172 and parameters: {'n_estimators': 556, 'max_depth': 17, 'min_samples_split': 18, 'min_samples_leaf': 3}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:53:46,969] Trial 26 finished with value: 0.7340463038585691 and parameters: {'n_estimators': 767, 'max_depth': 23, 'min_samples_split': 19, 'min_samples_leaf': 8}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:54:02,094] Trial 27 finished with value: 0.7332198758619288 and parameters: {'n_estimators': 328, 'max_depth': 23, 'min_samples_split': 15, 'min_samples_leaf': 9}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:54:34,877] Trial 28 finished with value: 0.7332280932497555 and parameters: {'n_estimators': 759, 'max_depth': 16, 'min_samples_split': 18, 'min_samples_leaf': 8}. Best is trial 14 with value: 0.7341967324480485.\n",
      "[I 2025-12-05 16:54:42,031] Trial 29 finished with value: 0.7319119002041562 and parameters: {'n_estimators': 158, 'max_depth': 16, 'min_samples_split': 17, 'min_samples_leaf': 9}. Best is trial 14 with value: 0.7341967324480485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor ROC AUC: 0.7342\n",
      "Mejores parámetros: {'n_estimators': 518, 'max_depth': 40, 'min_samples_split': 20, 'min_samples_leaf': 4}\n",
      "Mejor ROC AUC: 0.7342\n",
      "Desviación Estándar ROC AUC: 0.0029\n",
      "Accuracy del mejor modelo: 0.6718\n",
      "Desviación Estándar Accuracy: 0.0029\n",
      "Mejores parámetros: {'n_estimators': 518, 'max_depth': 40, 'min_samples_split': 20, 'min_samples_leaf': 4}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "target_column = 'is_popular'\n",
    "\n",
    "X = df_clas.drop(columns=[target_column])\n",
    "y = df_clas[target_column]\n",
    "\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, num_cols)     \n",
    "])\n",
    "def objective(trial):\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "    }\n",
    "    \n",
    "    model = RandomForestClassifier(\n",
    "        **param_grid, \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv=cv_inner, \n",
    "        scoring=['accuracy', 'roc_auc'],\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    mean_auc = cv_results['test_roc_auc'].mean()\n",
    "    mean_accuracy = cv_results['test_accuracy'].mean()\n",
    "\n",
    "    std_auc = cv_results['test_roc_auc'].std()\n",
    "    std_accuracy = cv_results['test_accuracy'].std()\n",
    "\n",
    "    trial.set_user_attr(\"accuracy\", mean_accuracy)\n",
    "    trial.set_user_attr(\"std_auc\", std_auc)     \n",
    "    trial.set_user_attr(\"std_accuracy\", std_accuracy)\n",
    "    \n",
    "    return mean_auc\n",
    "\n",
    "print(\"Iniciando optimización...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30) \n",
    "\n",
    "print(f\"Mejor ROC AUC: {study.best_value:.4f}\")\n",
    "print(f\"Mejores parámetros: {study.best_params}\")\n",
    "\n",
    "print(f\"Mejor ROC AUC: {study.best_value:.4f}\")\n",
    "print(f\"Desviación Estándar ROC AUC: {study.best_trial.user_attrs['std_auc']:.4f}\")\n",
    "\n",
    "print(f\"Accuracy del mejor modelo: {study.best_trial.user_attrs['accuracy']:.4f}\")\n",
    "print(f\"Desviación Estándar Accuracy: {study.best_trial.user_attrs['std_accuracy']:.4f}\")\n",
    "print(f\"Mejores parámetros: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d1b97",
   "metadata": {},
   "source": [
    "\n",
    "### Resultado Random Forest de clasificación:\n",
    "- Mejor ROC AUC: 0.7346\n",
    "- Desviación Estándar ROC AUC: 0.0030\n",
    "- Mejores parámetros:   {'n_estimators': 794, 'max_depth': 43, 'min_samples_split': 7, 'min_samples_leaf': 3}\n",
    "- Accuracy del mejor modelo: 0.6729\n",
    "- Desviación Estándar Accuracy: 0.0036"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b3168",
   "metadata": {},
   "source": [
    "## Por poco, pero el ganador es XGBoost con .7409 de ROC-AUC y .67 de accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50376717",
   "metadata": {},
   "source": [
    "## Regresion: Variable objetivo: Log-Shares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c643c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ec7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_clean['shares'], bins=50)\n",
    "plt.title(\"Distribución Original\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cd883",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg= df_clean.drop(columns=[\"shares\", \"is_popular\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe7e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_reg['log_shares'], bins=50)\n",
    "plt.title(\"Distribución Transformada\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ace041c",
   "metadata": {},
   "source": [
    "Hicimos una transformación logarítmica log-shares para que sean más rápidos los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e1313",
   "metadata": {},
   "source": [
    "\n",
    "### Random Forest de Regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "target_column = 'log_shares'\n",
    "\n",
    "X = df_reg.drop(columns=[target_column])\n",
    "y = df_reg[target_column]\n",
    "\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, num_cols)     \n",
    "])\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        **param_grid, \n",
    "        random_state=42, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "        pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv=cv_inner, \n",
    "        scoring=['r2'], \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    mean_r2 = cv_results['test_r2'].mean()\n",
    "    std_r2 = cv_results['test_r2'].std()\n",
    "\n",
    "    trial.set_user_attr(\"std_r2\", std_r2)\n",
    "    \n",
    "    return mean_r2\n",
    "\n",
    "print(\"Iniciando optimización para Regresión...\")\n",
    "study = optuna.create_study(direction='maximize') \n",
    "study.optimize(objective, n_trials=30) \n",
    "\n",
    "print(f\"Mejor R2 Score: {study.best_value:.4f}\")\n",
    "print(f\"Mejores parámetros: {study.best_params}\")\n",
    "print(f\"Desviación Estándar del R2: {study.best_trial.user_attrs['std_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "488099c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor R2 Score conseguido: 0.1647\n",
      "Mejores parámetros: {'n_estimators': 717, 'max_depth': 24, 'min_samples_split': 12, 'min_samples_leaf': 8}\n",
      "Desviación Estándar: 0.0085\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Mejor R2 Score conseguido: {study.best_value:.4f}\")\n",
    "print(f\"Mejores parámetros: {study.best_params}\")\n",
    "try:\n",
    "    print(f\"Desviación Estándar: {study.best_trial.user_attrs['std_r2']:.4f}\")\n",
    "except:\n",
    "    print(\"No se pudo recuperar la desviación estándar del mejor trial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84394e49",
   "metadata": {},
   "source": [
    "\n",
    "### Resultado Random Forest de Regresión:\n",
    "- Mejor R2 Score: 0.1647\n",
    "- Desviación Estándar ROC AUC: 0.0085\n",
    "- Mejores parámetros:   {'n_estimators': 717, 'max_depth': 24, 'min_samples_split': 12, 'min_samples_leaf': 8}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98040826",
   "metadata": {},
   "source": [
    "\n",
    "### XGboost de Regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8bee0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 14:51:44,996] A new study created in memory with name: no-name-5a01eb9b-61a1-4100-a33d-d64ae499d3db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando optimización para Regresión con XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-05 14:51:56,130] Trial 0 finished with value: 0.1738734000734372 and parameters: {'n_estimators': 633, 'max_depth': 4, 'learning_rate': 0.036328857202027984, 'subsample': 0.7486015740412126}. Best is trial 0 with value: 0.1738734000734372.\n",
      "[I 2025-12-05 14:52:01,797] Trial 1 finished with value: 0.1662852396251166 and parameters: {'n_estimators': 584, 'max_depth': 4, 'learning_rate': 0.09408386008135389, 'subsample': 0.9516335290485666}. Best is trial 0 with value: 0.1738734000734372.\n",
      "[I 2025-12-05 14:52:08,373] Trial 2 finished with value: 0.17309545593841064 and parameters: {'n_estimators': 946, 'max_depth': 3, 'learning_rate': 0.03931827615325188, 'subsample': 0.8235405185150035}. Best is trial 0 with value: 0.1738734000734372.\n",
      "[I 2025-12-05 14:52:19,849] Trial 3 finished with value: 0.07270298519088655 and parameters: {'n_estimators': 355, 'max_depth': 8, 'learning_rate': 0.16737147543727185, 'subsample': 0.6751685866908684}. Best is trial 0 with value: 0.1738734000734372.\n",
      "[I 2025-12-05 14:53:23,192] Trial 4 finished with value: 0.11699931893628919 and parameters: {'n_estimators': 722, 'max_depth': 11, 'learning_rate': 0.10218492669485257, 'subsample': 0.6357624739352602}. Best is trial 0 with value: 0.1738734000734372.\n",
      "[I 2025-12-05 14:53:38,010] Trial 5 finished with value: 0.14434932722745047 and parameters: {'n_estimators': 407, 'max_depth': 9, 'learning_rate': 0.06987799240719753, 'subsample': 0.7002503932027117}. Best is trial 0 with value: 0.1738734000734372.\n",
      "[I 2025-12-05 14:53:42,582] Trial 6 finished with value: 0.14793772866478866 and parameters: {'n_estimators': 292, 'max_depth': 7, 'learning_rate': 0.10540028754327281, 'subsample': 0.9116077608436388}. Best is trial 0 with value: 0.1738734000734372.\n",
      "[I 2025-12-05 14:53:44,079] Trial 7 finished with value: 0.1586157146261652 and parameters: {'n_estimators': 260, 'max_depth': 3, 'learning_rate': 0.2187795583425934, 'subsample': 0.7296307162144327}. Best is trial 0 with value: 0.1738734000734372.\n",
      "[I 2025-12-05 14:53:48,217] Trial 8 finished with value: 0.12831733109804344 and parameters: {'n_estimators': 557, 'max_depth': 5, 'learning_rate': 0.15295859617712845, 'subsample': 0.8165510755544021}. Best is trial 0 with value: 0.1738734000734372.\n",
      "[I 2025-12-05 14:53:52,467] Trial 9 finished with value: 0.15334226338850168 and parameters: {'n_estimators': 401, 'max_depth': 6, 'learning_rate': 0.09791291032956277, 'subsample': 0.8984873137741629}. Best is trial 0 with value: 0.1738734000734372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor R2 Score: 0.1739\n",
      "Mejores parámetros: {'n_estimators': 633, 'max_depth': 4, 'learning_rate': 0.036328857202027984, 'subsample': 0.7486015740412126}\n",
      "Desviación Estándar del R2: 0.0081\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "target_column = 'log_shares'\n",
    "\n",
    "X = df_reg.drop(columns=[target_column])\n",
    "y = df_reg[target_column]\n",
    "\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, num_cols)     \n",
    "])\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0)\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        **param_grid, \n",
    "        random_state=42, \n",
    "        n_jobs=-1,\n",
    "        objective='reg:squarederror'\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "        pipeline, \n",
    "        X, \n",
    "        y, \n",
    "        cv=cv_inner, \n",
    "        scoring=['r2'], \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    mean_r2 = cv_results['test_r2'].mean()\n",
    "    std_r2 = cv_results['test_r2'].std()\n",
    "\n",
    "    trial.set_user_attr(\"std_r2\", std_r2)\n",
    "    \n",
    "    return mean_r2\n",
    "\n",
    "print(\"Iniciando optimización para Regresión con XGBoost...\")\n",
    "study = optuna.create_study(direction='maximize') \n",
    "study.optimize(objective, n_trials=10) \n",
    "\n",
    "print(f\"Mejor R2 Score: {study.best_value:.4f}\")\n",
    "print(f\"Mejores parámetros: {study.best_params}\")\n",
    "print(f\"Desviación Estándar del R2: {study.best_trial.user_attrs['std_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07963e2d",
   "metadata": {},
   "source": [
    "\n",
    "### Resultado XGboost de Regresión:\n",
    "- Mejor R2 Score: 0.1739\n",
    "- Desviación Estándar ROC AUC: 0.0081\n",
    "- Mejores parámetros:  {'n_estimators': 633, 'max_depth': 4, 'learning_rate': 0.036328857202027984, 'subsample': 0.7486015740412126}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2b98d6",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "El mejor resultado corresponde al modelo XGBoost de regresión, que alcanzó un R² de 0.1739, superando ligeramente a Random Forest. Este valor indica que menos del 18% de la variabilidad en el número de shares puede ser explicada por las características disponibles. Esto sugiere que, a pesar de la riqueza del dataset, existen factores externos al contenido textual, metadatos o rendimiento histórico de keywords que influyen significativamente en la difusión real de una noticia. La baja capacidad explicativa también es consistente con la alta dispersión del comportamiento viral, fenómeno que suele responder a dinámicas sociales impredecibles, efectos de red, temporalidad y exposición inicial elementos no capturados por el conjunto actual de predictores.\n",
    "\n",
    "Por otra parte, los resultados de clasificación muestran un desempeño más satisfactorio. El modelo XGBoost de clasificación obtuvo el mejor ROC AUC: 0.7411 y un accuracy de 0.6781, superando a Random Forest. Un ROC AUC por encima de 0.74 indica una capacidad moderada para distinguir entre artículos virales y no virales, lo cual refleja que, aunque predecir el número exacto de shares es complejo, sí existen patrones identificables que permiten clasificar si un artículo superará la mediana de viralidad. Variables relacionadas con el tema, temporalidad y desempeño histórico de keywords parecen aportar información suficiente para esta tarea, aunque no para predecir la magnitud exacta del impacto.\n",
    "\n",
    "Además, las desviaciones estándar bajas en ambos enfoques evidencian estabilidad en las estimaciones, lo que demuestra que el uso de Stratified K-Folds fue adecuado para obtener métricas consistentes y evitar dependencia de una sola partición del dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
