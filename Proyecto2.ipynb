{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56eb4c94",
   "metadata": {},
   "source": [
    "# Proyecto 2 laboratorio de aprendizaje estadístico\n",
    "\n",
    "- Dayanni Godoy Rosales\n",
    "- Mateo Tavares Trueba\n",
    "- Jesús Cortes Flores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bea16e",
   "metadata": {},
   "source": [
    "## Objetivos del Proyecto \n",
    "\n",
    "### Objetivo General\n",
    "\n",
    "Construir y evaluar modelos de **Clasificación Binaria** (Regresión Logística, SVM, y Redes Neuronales) utilizando datos del censo (Adult Dataset) para **predecir si el ingreso anual de un individuo es superior a $\\$$50,000** ($\\mathbf{>50K}$).\n",
    "\n",
    "---\n",
    "\n",
    "### Objetivos Específicos\n",
    "\n",
    "1.  **Preparación de Datos:** Realizar la limpieza, el preprocesamiento y la transformación de las variables categóricas (mediante *One-Hot Encoding*) y el escalado de las variables continuas para adecuarlas a los modelos de clasificación.\n",
    "\n",
    "2.  **Establecimiento de *Baseline*:** Implementar el modelo de **Regresión Logística** como línea base y registrar sus métricas de rendimiento (Accuracy, Precision, Recall, F1-Score) utilizando el umbral de clasificación por defecto (0.5).\n",
    "\n",
    "3.  **Modelado Avanzado:** Implementar y entrenar los modelos de **Máquinas de Vectores de Soporte (SVM)**, explorando el uso de diferentes **Kernels**, y el modelo de **Red Neuronal (MLP)**.\n",
    "\n",
    "4.  **Optimización de Hiperparámetros:** Utilizar la técnica de **Optimización Bayesiana** para sintonizar los hiperparámetros clave de los modelos (especialmente SVM y MLP) con el fin de mejorar su rendimiento y robustez generalizada.\n",
    "\n",
    "5.  **Análisis y Conclusiones:** Comparar el rendimiento de todos los modelos (Baseline, SVM, MLP) antes y después de la optimización del umbral, y documentar la relación entre el *Recall* obtenido y el *Costo* en la **Precisión** del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887509d",
   "metadata": {},
   "source": [
    "## Marco Teórico \n",
    "\n",
    "---\n",
    "\n",
    "### Regresión Logística\n",
    "\n",
    "La **Regresión Logística** es un método estadístico fundamental utilizado para la **clasificación binaria** (aunque puede extenderse a multiclase). A pesar de su nombre, no predice un valor continuo, sino la **probabilidad** de que una instancia pertenezca a una clase en particular. Utiliza la **función sigmoide** (o función logística) para transformar la salida lineal de las características de entrada en una probabilidad que siempre estará acotada entre 0 y 1. El objetivo del modelo es encontrar los coeficientes óptimos que maximicen la verosimilitud (o minimicen la pérdida logarítmica) de estas probabilidades.\n",
    "\n",
    "---\n",
    "\n",
    "### Máquinas de Vectores de Soporte (Support Vector Machines - SVM)\n",
    "\n",
    "Las **Máquinas de Vectores de Soporte (SVM)** son un clasificador discriminativo que busca encontrar el **hiperplano** óptimo que separa las clases en un espacio de características. Lo que hace que el SVM sea potente es que se enfoca en maximizar el **margen** de separación, es decir, la distancia entre el hiperplano y las instancias de datos más cercanas de cada clase, conocidas como **vectores de soporte**.  Esta maximización del margen tiende a mejorar la capacidad de generalización del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### Redes Neuronales (Multi-layer Perceptron - MLP)\n",
    "\n",
    "Una **Red Neuronal Artificial (RNA)**, como el **Perceptrón Multicapa (MLP)**, es un modelo que simula la estructura del cerebro. Se compone de nodos (neuronas) organizados en capas: una **capa de entrada**, una o más **capas ocultas** y una **capa de salida**. El MLP es capaz de aprender relaciones no lineales complejas gracias a las **funciones de activación** aplicadas en cada neurona. El proceso de entrenamiento clave es la **retropropagación (*backpropagation*)**, que ajusta los pesos de la red minimizando la función de pérdida a través del descenso de gradiente.\n",
    "\n",
    "---\n",
    "\n",
    "### Kernels (Funciones Kernel)\n",
    "\n",
    "Los **Kernels** son funciones matemáticas utilizadas principalmente en los SVM para manejar problemas de clasificación **no linealmente separables**. El **Truco del Kernel (*Kernel Trick*)** permite que el algoritmo trabaje en un espacio de características de mayor dimensión (donde los datos son linealmente separables) sin tener que calcular explícitamente la transformación de las coordenadas. Esto reduce drásticamente el costo computacional. Ejemplos comunes incluyen el kernel Lineal, Polinomial y la Función de Base Radial (RBF).\n",
    "\n",
    "---\n",
    "\n",
    "### Métricas para Clasificación\n",
    "\n",
    "Para evaluar el rendimiento de un modelo de clasificación, no basta con una sola medida. Se utilizan **métricas** que cuantifican diferentes aspectos de su desempeño:\n",
    "* **Precisión (*Accuracy*):** El porcentaje de predicciones correctas totales. Es útil, pero engañoso en casos de clases desbalanceadas.\n",
    "* **Matriz de Confusión:** Tabla que desglosa los resultados en Verdaderos Positivos (VP), Falsos Positivos (FP), Verdaderos Negativos (VN) y Falsos Negativos (FN).\n",
    "* **Precision y Recall:**\n",
    "    * **Precision:** De todos los positivos predichos, cuántos fueron correctos ($\\frac{VP}{VP+FP}$).\n",
    "    * **Recall:** De todos los positivos reales, cuántos fueron detectados correctamente ($\\frac{VP}{VP+FN}$).\n",
    "* **F1-Score:** Media armónica de Precision y Recall, útil para buscar un equilibrio.\n",
    "\n",
    "---\n",
    "\n",
    "### Hiperparámetros\n",
    "\n",
    "Los **Hiperparámetros** son variables de configuración externas que se establecen **antes** de comenzar el entrenamiento de un modelo (a diferencia de los parámetros, que son aprendidos, como los pesos). Su correcta elección es vital, ya que controlan el proceso de aprendizaje del modelo y tienen un impacto directo en su rendimiento y en evitar el sobreajuste (*overfitting*). Ejemplos incluyen el coeficiente de regularización $C$ en SVM, la tasa de aprendizaje en Redes Neuronales.\n",
    "\n",
    "---\n",
    "\n",
    "### Optimización Bayesiana\n",
    "\n",
    "La **Optimización Bayesiana** es una técnica de optimización secuencial y global que se utiliza para encontrar el conjunto óptimo de **hiperparámetros** de un modelo de forma más eficiente que las búsquedas ingenuas (como *Grid Search* o *Random Search*). Este método construye un **modelo probabilístico** (generalmente un Proceso Gaussiano) del rendimiento de la función objetivo (la métrica de evaluación), utilizando esta información para seleccionar de manera inteligente el próximo conjunto de hiperparámetros a probar, minimizando así las evaluaciones costosas del modelo completo.\n",
    "\n",
    "---\n",
    "\n",
    "### Tema Elegido para Proyecto\n",
    "\n",
    "**Census Income**\n",
    "\n",
    "En este proyecto, la aplicación de estos modelos de clasificación se centrará en **predecir si el ingreso anual de un individuo supera los 50,000**. Específicamente, se busca construir un modelo que prediga si el **ingreso anual de un individuo supera los $\\$$50,000** ($\\mathbf{>50K}$), utilizando datos demográficos y laborales provenientes del censo de 1994 (Dataset \"Adult\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87110081",
   "metadata": {},
   "source": [
    "## Análisis del Dataset\n",
    "\n",
    "### 1. ¿De dónde viene?\n",
    "**Fuente Original:** Base de datos de la Oficina del Censo de Estados Unidos (1994).\n",
    "* **Link:** https://archive.ics.uci.edu/dataset/20/census+income\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ¿Qué contiene?\n",
    "* El dataset contiene aproximadamente **48,842 instancias** (registros), divididas en un conjunto de entrenamiento (`adult.data`, 32,561) y un conjunto de prueba (`adult.test`, 16,281).\n",
    "* Se compone de **14 características predictoras** y **1 variable objetivo** (ingreso).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ¿Qué información dan las muestras?\n",
    "Cada muestra (fila) representa a un individuo y proporciona características demográficas y laborales.\n",
    "* **Variables Continuas:** `age`, `fnlwgt` (Final Weight), `education-num` (Nivel educativo numérico), `capital-gain`, `capital-loss`, `hours-per-week`.\n",
    "* **Variables Categóricas:** `workclass`, `education`, `marital-status`, `occupation`, `relationship`, `race`, `sex`, `native-country`.\n",
    "* **Variable Objetivo:** `income` ($>50K$ o $\\leq50K$).\n",
    "#### A. Descripción de Variables\n",
    "\n",
    "| Variable | Tipo | Significado | Ejemplos de Categorías (si aplica) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **age** | Continua | Edad del individuo. | N/A |\n",
    "| **workclass** | Categórica | Tipo de empleador o sector de trabajo. | Private, Self-emp-not-inc, Federal-gov, Local-gov, State-gov, Without-pay. |\n",
    "| **fnlwgt** | Continua | Peso de la muestra (Final Weight). Indica cuántas personas representa la muestra en el censo. (A menudo se omite en el modelado predictivo). | N/A |\n",
    "| **education** | Categórica | Nivel educativo formal alcanzado. | Bachelors, HS-grad, Some-college, Masters, Doctorate. |\n",
    "| **education-num** | Continua | Valor numérico que corresponde al nivel de `education`. (P. ej., Masters=14, Bachelors=13). | N/A |\n",
    "| **marital-status** | Categórica | Estado civil. | Married-civ-spouse, Divorced, Never-married, Widowed. |\n",
    "| **occupation** | Categórica | Ocupación laboral del individuo. | Exec-managerial, Prof-specialty, Sales, Craft-repair, Adm-clerical. |\n",
    "| **relationship** | Categórica | Relación con el cabeza de familia. | Husband, Wife, Own-child, Not-in-family, Other-relative. |\n",
    "| **race** | Categórica | Raza del individuo. | White, Black, Asian-Pac-Islander, Amer-Indian-Eskimo. |\n",
    "| **sex** | Categórica | Género. | Female, Male. |\n",
    "| **capital-gain** | Continua | Ganancias de capital reportadas (ingresos por inversiones, distintos del salario). | N/A |\n",
    "| **capital-loss** | Continua | Pérdidas de capital reportadas. | N/A |\n",
    "| **hours-per-week** | Continua | Horas trabajadas por semana. | N/A |\n",
    "| **native-country** | Categórica | País de origen o nacimiento. | United-States, Mexico, Philippines, Germany (con muchos valores únicos). |\n",
    "| **income (Target)** | Binaria | **Variable Objetivo:** Ingreso anual. | **>50K** (Clase Positiva) o **<=50K** (Clase Negativa). |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ¿Qué se quiere analizar?\n",
    "* El objetivo principal es construir un **modelo predictivo** que pueda determinar, basándose en los atributos demográficos y laborales de un individuo, si su ingreso anual es superior a $50,000.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ¿Qué variables se tienen que transformar para usarse en regresión?\n",
    "Para usar la mayoría de los modelos de clasificación (Regresión Logística, MLP, SVM) que trabajan con datos numéricos, todas las **variables categóricas** deben ser transformadas:\n",
    "* `workclass`\n",
    "* `education`\n",
    "* `marital-status`\n",
    "* `occupation`\n",
    "* `relationship`\n",
    "* `race`\n",
    "* `sex`\n",
    "* `native-country`\n",
    "* La variable objetivo `income`.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ¿Qué transformaciones se van a usar?\n",
    "\n",
    "#### A. Codificación de Variables Categóricas (Características)\n",
    "* **One-Hot Encoding (OHE) :** Se usará para variables nominales (p. ej., `workclass`,`education`, `marital-status`, `occupation`, `relationship`, `race`, `sex` y `native-country`). Convierte una columna categórica con $k$ categorías únicas en $k$ nuevas columnas binarias (0 o 1). \n",
    "\n",
    "\n",
    "#### B. Transformación de Variables Continuas\n",
    "* **Estandarización/Normalización:** Las variables continuas (`age`,`education-num`, `capital-gain`, `capital-loss`, `hours-per-week`, ) deben ser **escaladas** (Estandarización o Normalización) para evitar que las variables con mayor rango dominen el cálculo de distancias o la optimización.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ¿Qué resultado se podría encontrar al realizar una clasificación?\n",
    "Al construir el modelo, se espera encontrar:\n",
    "* **Modelo Predictivo:** Un modelo capaz de predecir la probabilidad de que un individuo gane más de $50,000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba06bc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\dayan\\anaconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from ucimlrepo) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from ucimlrepo) (2025.10.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d5d5cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dayan\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.9.0-cp312-cp312-win_amd64.whl (109.3 MB)\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.8/109.3 MB 14.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 3.1/109.3 MB 10.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 5.5/109.3 MB 10.8 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 8.1/109.3 MB 11.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 10.0/109.3 MB 10.9 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 11.8/109.3 MB 10.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 9.8 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 14.7/109.3 MB 9.4 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 16.3/109.3 MB 9.2 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 17.8/109.3 MB 9.1 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 19.7/109.3 MB 8.9 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 21.2/109.3 MB 8.8 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 22.8/109.3 MB 8.8 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 24.6/109.3 MB 8.7 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 26.2/109.3 MB 8.7 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 8.6 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 29.6/109.3 MB 8.7 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 31.5/109.3 MB 8.6 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 33.0/109.3 MB 8.6 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 34.9/109.3 MB 8.6 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 36.7/109.3 MB 8.6 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 38.3/109.3 MB 8.6 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 40.1/109.3 MB 8.6 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 41.9/109.3 MB 8.6 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 43.8/109.3 MB 8.6 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 45.6/109.3 MB 8.6 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 47.4/109.3 MB 8.6 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 49.5/109.3 MB 8.6 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 51.4/109.3 MB 8.7 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 53.2/109.3 MB 8.7 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 55.3/109.3 MB 8.7 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 57.1/109.3 MB 8.7 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 59.0/109.3 MB 8.7 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 61.1/109.3 MB 8.8 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 63.2/109.3 MB 8.8 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 65.0/109.3 MB 8.8 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 67.1/109.3 MB 8.9 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 69.2/109.3 MB 8.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 71.3/109.3 MB 8.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 73.4/109.3 MB 8.9 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 75.5/109.3 MB 9.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 77.6/109.3 MB 9.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 79.7/109.3 MB 9.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 81.8/109.3 MB 9.1 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 84.1/109.3 MB 9.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 86.2/109.3 MB 9.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 88.6/109.3 MB 9.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 90.7/109.3 MB 9.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 92.8/109.3 MB 9.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 94.9/109.3 MB 9.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 96.7/109.3 MB 9.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 98.8/109.3 MB 9.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 100.9/109.3 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 103.0/109.3 MB 9.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 105.1/109.3 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  107.2/109.3 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.1/109.3 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.1/109.3 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 109.3/109.3 MB 9.1 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.3 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 7.4 MB/s eta 0:00:00\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.14.0 torch-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d760687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import optuna\n",
    "\n",
    "from optuna.samplers import GPSampler \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29676ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'adult.data' cargado en df_completo.\n",
      "\n",
      "División completada:\n",
      "Forma de X_train (para K-Fold y Optuna): (26048, 14)\n",
      "Forma de y_train (para K-Fold y Optuna): (26048,)\n",
      "Forma de X_test (para el reporte final): (6513, 14)\n",
      "Forma de y_test (para el reporte final): (6513,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "column_names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "    'income'  \n",
    "]\n",
    "\n",
    "# 1. Cargamos solo train porque son demasiados datos\n",
    "df_completo = pd.read_csv(\n",
    "    'adult.data',        \n",
    "    header=None,        \n",
    "    names=column_names,\n",
    "    na_values=\" ?\"\n",
    ")\n",
    "df_completo = df_completo.dropna(subset=['income'])\n",
    "print(\"Dataset 'adult.data' cargado en df_completo.\")\n",
    "\n",
    "def clean_income(value):\n",
    "    if '<=50K' in value: return 0\n",
    "    elif '>50K' in value: return 1\n",
    "    return np.nan\n",
    "\n",
    "df_completo['income'] = df_completo['income'].apply(clean_income)\n",
    "df_completo = df_completo.dropna(subset=['income'])\n",
    "\n",
    "X_completo = df_completo.drop('income', axis=1)\n",
    "X_completo = df_completo.drop('fnlwgt', axis=1)\n",
    "y_completo = df_completo['income'].astype(int)\n",
    "\n",
    "# Dividimos el dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_completo, \n",
    "    y_completo, \n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y_completo \n",
    ")\n",
    "print(\"\\nDivisión completada:\")\n",
    "print(\"Forma de X_train (para K-Fold y Optuna): \" + str(X_train.shape))\n",
    "print(\"Forma de y_train (para K-Fold y Optuna): \" + str(y_train.shape))\n",
    "print(\"Forma de X_test (para el reporte final): \" + str(X_test.shape))\n",
    "print(\"Forma de y_test (para el reporte final): \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8cad0407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando EDA ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       30725 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  int64 \n",
      " 5   marital-status  32561 non-null  object\n",
      " 6   occupation      30718 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hours-per-week  32561 non-null  int64 \n",
      " 13  native-country  31978 non-null  object\n",
      " 14  income          32561 non-null  int64 \n",
      "dtypes: int64(7), object(8)\n",
      "memory usage: 3.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "      <td>0.240810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "      <td>0.427581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
       "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
       "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
       "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week        income  \n",
       "count    32561.000000  32561.000000  \n",
       "mean        40.437456      0.240810  \n",
       "std         12.347429      0.427581  \n",
       "min          1.000000      0.000000  \n",
       "25%         40.000000      0.000000  \n",
       "50%         40.000000      0.000000  \n",
       "75%         45.000000      0.000000  \n",
       "max         99.000000      1.000000  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--- Iniciando EDA ---\")\n",
    "\n",
    "df_completo.head()\n",
    "\n",
    "df_completo.info()\n",
    "\n",
    "df_completo.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc6966",
   "metadata": {},
   "source": [
    "## Ingeniería de Características (Feature Engineering)\n",
    "Aquí preparamos los datos para el modelo. Debemos:\n",
    "\n",
    "Convertir la y (income) a 0 y 1.\n",
    "\n",
    "Manejar los valores nulos (Imputar).\n",
    "\n",
    "Convertir categóricas a números (One-Hot Encoding).\n",
    "\n",
    "Escalar las numéricas (Standard Scaler).\n",
    "\n",
    "Usaremos un Pipeline y ColumnTransformer para hacerlo de forma limpia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab124559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocesador y K-Fold (k=5) listos.\n",
      "--- Configuración Común Lista ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_features = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "cat_features = ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, num_features),\n",
    "        ('cat', categorical_transformer, cat_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"\\nPreprocesador y K-Fold (k=5) listos.\")\n",
    "print(\"--- Configuración Común Lista ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd9c76",
   "metadata": {},
   "source": [
    "## Regresión Logística usando optimización bayesiana (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b00997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-02 13:53:22,071] A new study created in memory with name: no-name-a35e1e54-16f4-4168-aefb-ebed40faffce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando Optimización: Regresión Logística \n",
      "Iniciando estudio (n_trials=20) sobre 26048 filas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-02 13:54:12,385] Trial 0 finished with value: 0.9045913452631156 and parameters: {'C': 0.1057017827442291, 'l1_ratio': 0.10355993126725827}. Best is trial 0 with value: 0.9045913452631156.\n",
      "[I 2025-11-02 13:55:12,915] Trial 1 finished with value: 0.9008187559149899 and parameters: {'C': 0.00955477145252365, 'l1_ratio': 0.14686031127964838}. Best is trial 0 with value: 0.9045913452631156.\n",
      "[I 2025-11-02 13:55:13,500] Trial 2 finished with value: 0.5 and parameters: {'C': 0.00022598242981845454, 'l1_ratio': 0.8149561298096853}. Best is trial 0 with value: 0.9045913452631156.\n",
      "[I 2025-11-02 13:56:02,790] Trial 3 finished with value: 0.9033341220445493 and parameters: {'C': 0.04026927740202841, 'l1_ratio': 0.7494544125199921}. Best is trial 0 with value: 0.9045913452631156.\n",
      "[I 2025-11-02 13:57:36,372] Trial 4 finished with value: 0.9044348596700784 and parameters: {'C': 69.06063660507722, 'l1_ratio': 0.25540904962263067}. Best is trial 0 with value: 0.9045913452631156.\n",
      "[I 2025-11-02 13:59:31,099] Trial 5 finished with value: 0.9045788405022522 and parameters: {'C': 6.421094212301918, 'l1_ratio': 0.17976783594417967}. Best is trial 0 with value: 0.9045913452631156.\n",
      "[I 2025-11-02 14:01:09,572] Trial 6 finished with value: 0.9047310568336361 and parameters: {'C': 0.2584563365588632, 'l1_ratio': 0.2749977246404235}. Best is trial 6 with value: 0.9047310568336361.\n",
      "[I 2025-11-02 14:01:19,443] Trial 7 finished with value: 0.9048039583282808 and parameters: {'C': 0.4291554160942694, 'l1_ratio': 0.03775165405789094}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:02:22,619] Trial 8 finished with value: 0.9044549199445845 and parameters: {'C': 0.16343481922436434, 'l1_ratio': 0.7799482453454848}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:04:05,784] Trial 9 finished with value: 0.9046335044193908 and parameters: {'C': 0.2700569135782735, 'l1_ratio': 0.6205166067295861}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:05:24,730] Trial 10 finished with value: 0.9044186515396347 and parameters: {'C': 99.99999999999987, 'l1_ratio': 1.0}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:05:46,806] Trial 11 finished with value: 0.9034519452868176 and parameters: {'C': 0.021648488976572542, 'l1_ratio': 0.0}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:07:21,590] Trial 12 finished with value: 0.904464658532768 and parameters: {'C': 17.088558962474725, 'l1_ratio': 0.9999999999999999}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:09:11,071] Trial 13 finished with value: 0.9046728129677192 and parameters: {'C': 2.273190488558465, 'l1_ratio': 1.0}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:09:31,179] Trial 14 finished with value: 0.9047246986114701 and parameters: {'C': 1.8124634334747032, 'l1_ratio': 0.0}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:14:10,665] Trial 15 finished with value: 0.9044884935562056 and parameters: {'C': 22.598873832139322, 'l1_ratio': 0.0}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:14:32,055] Trial 16 finished with value: 0.9044299493254041 and parameters: {'C': 100.0, 'l1_ratio': 0.0}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:14:46,527] Trial 17 finished with value: 0.89849652880314 and parameters: {'C': 0.01598160991877054, 'l1_ratio': 0.9999999999999999}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:15:07,857] Trial 18 finished with value: 0.9047575524728109 and parameters: {'C': 0.8155582857138568, 'l1_ratio': 1.0}. Best is trial 7 with value: 0.9048039583282808.\n",
      "[I 2025-11-02 14:15:24,224] Trial 19 finished with value: 0.9047717215130078 and parameters: {'C': 1.0202232195303325, 'l1_ratio': 0.4450636246493821}. Best is trial 7 with value: 0.9048039583282808.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimización de Regresión Logística Completada\n",
      "Mejor score (AUC promedio): 0.9048039583282808\n",
      "Mejores Hiperparámetros: {'C': 0.4291554160942694, 'l1_ratio': 0.03775165405789094}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nIniciando Optimización: Regresión Logística \")\n",
    "\n",
    "def objective_logreg(trial):\n",
    "    C = trial.suggest_float('C', 1e-4, 1e2, log=True)\n",
    "    l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "    \n",
    "    model = LogisticRegression(\n",
    "        penalty='elasticnet',\n",
    "        solver='saga',\n",
    "        C=C,\n",
    "        l1_ratio=l1_ratio,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "    \n",
    "    auc_promedio = scores.mean()\n",
    "    return auc_promedio\n",
    "\n",
    "# Iniciar el estudio de Optuna \n",
    "n_trials_logreg = 20\n",
    "print(\"Iniciando estudio (n_trials=\" + str(n_trials_logreg) + \") sobre \" + str(X_train.shape[0]) + \" filas.\")\n",
    "\n",
    "sampler = GPSampler()\n",
    "study_logreg = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study_logreg.optimize(objective_logreg, n_trials=n_trials_logreg)\n",
    "# Resultados \n",
    "print(\"\\nOptimización de Regresión Logística Completada\")\n",
    "print(\"Mejor score (AUC promedio): \" + str(study_logreg.best_value))\n",
    "print(\"Mejores Hiperparámetros: \" + str(study_logreg.best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2cc8d",
   "metadata": {},
   "source": [
    "### Mejor regresión logística según la optimización realizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study_logreg.best_params\n",
    "\n",
    "# Modelo optimizado final de Regresión Logística\n",
    "final_logreg_model = LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',\n",
    "    C=best_params['C'], \n",
    "    l1_ratio=best_params['l1_ratio'], \n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline final\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', final_logreg_model)\n",
    "])\n",
    "\n",
    "# Modelo final con todos los datos de X_train\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en X_test\n",
    "y_pred = final_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d96b8",
   "metadata": {},
   "source": [
    "### **Obtener y_pred y y_prob**\n",
    "\n",
    "**y_pred** = final_pipeline.predict(X_test)\n",
    "\n",
    "**Qué es:** Las decisiones finales 0 o 1 (basadas en un umbral de 0.5).\n",
    "\n",
    "**Para qué sirve:** Para calcular la Matriz de Confusión y, a partir de ella, la Precisión, el Recall y el Accuracy.\n",
    "\n",
    "- Si le das los datos de un nuevo cliente, y_pred te dirá la decisión: \"0\" (predecimos que gana <=50K) o \"1\" (predecimos que gana >50K).\n",
    "\n",
    "\n",
    "**y_prob** = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "**Qué es:** Las probabilidades (ej. 0.15, 0.78, 0.55).\n",
    "\n",
    "Para qué sirve: Para calcular el AUC (que evalúa todas las probabilidades sin un umbral fijo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb501ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Obteniendo y_pred (Predicciones de Clase 0 o 1) ---\n",
      "Primeras 10 predicciones (y_pred):\n",
      "[0 0 1 0 0 0 0 1 0 0]\n",
      "\n",
      "--- 2. Obteniendo y_prob (Predicciones de Probabilidad) ---\n",
      "Primeras 10 probabilidades (y_prob) para la clase 1:\n",
      "[0.118 0.013 0.992 0.5   0.123 0.01  0.022 0.529 0.341 0.091]\n",
      "\n",
      "--- 3. Usando y_pred y y_prob para Evaluación ---\n",
      "\n",
      "Evaluación con 'y_prob':\n",
      "AUC (Area Under Curve): 0.9078124677575783\n",
      "\n",
      "Evaluación con 'y_pred' (umbral 0.5):\n",
      "Accuracy (Exactitud): 0.8545984953170582\n",
      "\n",
      "Reporte de Clasificación (Precisión, Recall, etc.):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.91      4945\n",
      "           1       0.74      0.61      0.67      1568\n",
      "\n",
      "    accuracy                           0.85      6513\n",
      "   macro avg       0.81      0.77      0.79      6513\n",
      "weighted avg       0.85      0.85      0.85      6513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Obteniendo y_pred (Predicciones de Clase 0 o 1)\")\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Primeras 10 predicciones (y_pred):\")\n",
    "print(str(y_pred[:10]))\n",
    "\n",
    "\n",
    "print(\"\\n--- 2. Obteniendo y_prob (Predicciones de Probabilidad) ---\")\n",
    "y_prob = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Primeras 10 probabilidades (y_prob) para la clase 1:\")\n",
    "print(str(np.round(y_prob[:10], 3)))\n",
    "\n",
    "print(\"\\nUsando y_pred y y_prob para Evaluación\")\n",
    "\n",
    "# Probabilidades: ROC AUC: evalúa qué tan bien discrimina el modelo en los umbrales\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"\\nEvaluación con 'y_prob':\")\n",
    "print(\"AUC (Area Under Curve): \" + str(auc))\n",
    "\n",
    "\n",
    "# Clases 0/1: Accuracy, Precision, Recall: Evalúan el rendimiento con el umbral de 0.5\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"\\nEvaluación con 'y_pred' (umbral 0.5):\")\n",
    "print(\"Accuracy (Exactitud): \" + str(accuracy))\n",
    "print(\"\\nReporte de Clasificación (Precisión, Recall, etc.):\")\n",
    "print(str(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61700176",
   "metadata": {},
   "source": [
    "### **Interpretación:** \n",
    "El modelo es muy bueno para discriminar entre las dos clases. Si tomamos a una persona al azar que gana >50K (clase 1) y otra que gana <=50K (clase 0), hay un 90.3% de probabilidad de que el modelo le asigne una probabilidad más alta a la persona correcta.\n",
    "\n",
    "- **Accuracy: 0.850** El 85% de tus predicciones totales en el set de prueba fueron correctas.\n",
    "\n",
    "- **Clase 0 (<=50K, la mayoría):**\n",
    "EL modelo es excelente prediciendo a la gente que gana poco. Los encuentra (recall) y acierta cuando lo dice (precision).\n",
    "- **Clase 1 (>50K, la minoría):** \n",
    "Cuando el modelo dice \"1\" (>50K), acierta el 73% de las veces. El modelo solo logra encontrar al 59% de todas las personas que realmente ganan >50K. El otro 41% los clasifica mal como \"0\" (son Falsos Negativos).\n",
    "\n",
    "**Conclusión:**  Si el objetivo del modelo fuera encontrar todos los posibles clientes de bajos ingresos, funciona perfectamente, Recall de 93%.\n",
    "\n",
    "Si fuera lo contrario, habría que bajar el umbral ya que el dataset es desbalanceado (hay muchas más personas \"0\" que \"1\", 12435 vs 3846). El modelo aprende que \"decir 0\" es una apuesta segura para subir el Accuracy general. \n",
    "\n",
    "La mayoría de los problemas del mundo real (detección de fraude, diagnósticos médicos, etc.) son desbalanceados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27507566",
   "metadata": {},
   "source": [
    "## SVM con kernel RBF con optimización de hiperparámetros Optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbcbe889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-02 11:45:51,098] A new study created in memory with name: no-name-62ba4f4f-22a7-41df-85e7-5f622ab5cb8e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando Optimización: SVC Kernel RBF con GP\n",
      "Iniciando estudio (n_trials=5) usando un sampler GP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-02 11:50:12,680] Trial 0 finished with value: 0.9052615536516129 and parameters: {'C': 34.18647366422358, 'gamma': 0.000918039711950677}. Best is trial 0 with value: 0.9052615536516129.\n",
      "[I 2025-11-02 11:54:15,532] Trial 1 finished with value: 0.8999329831704556 and parameters: {'C': 13.01580550549988, 'gamma': 0.031232973387632697}. Best is trial 0 with value: 0.9052615536516129.\n",
      "[I 2025-11-02 11:59:54,214] Trial 2 finished with value: 0.8884702103578643 and parameters: {'C': 34.61959428738803, 'gamma': 0.06414011710202464}. Best is trial 0 with value: 0.9052615536516129.\n",
      "[I 2025-11-02 12:03:41,841] Trial 3 finished with value: 0.9068831401620987 and parameters: {'C': 0.4279788460140292, 'gamma': 0.03206585557249091}. Best is trial 3 with value: 0.9068831401620987.\n",
      "[I 2025-11-02 12:07:31,183] Trial 4 finished with value: 0.906527577132002 and parameters: {'C': 0.3746278086948534, 'gamma': 0.043478116530024986}. Best is trial 3 with value: 0.9068831401620987.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¡Optimización de SVC (RBF) con GP Completada!\n",
      "Mejor score (AUC promedio): 0.9068831401620987\n",
      "Mejores Hiperparámetros: {'C': 0.4279788460140292, 'gamma': 0.03206585557249091}\n"
     ]
    }
   ],
   "source": [
    "# Definición de la Función Objetivo \n",
    "print(\"\\nIniciando Optimización: SVC Kernel RBF con GP\")\n",
    "\n",
    "def objective_svc_rbf(trial):\n",
    "    # 'C' (regularización) en escala logarítmica\n",
    "    C = trial.suggest_float('C', 1e-2, 1e2, log=True)\n",
    "    \n",
    "    # 'gamma' (coeficiente del kernel) en escala logarítmica\n",
    "    gamma = trial.suggest_float('gamma', 1e-4, 1e-1, log=True)\n",
    "    \n",
    "    # Creamos el modelo\n",
    "    model = SVC(\n",
    "        kernel='rbf',\n",
    "        C=C,\n",
    "        gamma=gamma,\n",
    "        probability=True, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Pipeline completo \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Validamos con K-Fold y calculamos el AUC \n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "    \n",
    "    # Devolvemos el AUC promedio \n",
    "    auc_promedio = scores.mean()\n",
    "    return auc_promedio\n",
    "\n",
    "# Iniciar el estudio de Optuna con GPSampler\n",
    "n_trials_svc = 5\n",
    "print(\"Iniciando estudio (n_trials=\" + str(n_trials_svc) + \") usando un sampler GP...\")\n",
    "\n",
    "sampler = GPSampler() \n",
    "study_svc = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "# Ejecutamos la optimización\n",
    "study_svc.optimize(objective_svc_rbf, n_trials=n_trials_svc)\n",
    "\n",
    "# Resultados\n",
    "print(\"\\n¡Optimización de SVC (RBF) con GP Completada!\")\n",
    "print(\"Mejor score (AUC promedio): \" + str(study_svc.best_value))\n",
    "print(\"Mejores Hiperparámetros: \" + str(study_svc.best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb778c54",
   "metadata": {},
   "source": [
    "### Mejor SVM con kernel RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63c337cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros encontrados: {'C': 0.4279788460140292, 'gamma': 0.03206585557249091}\n",
      "Entrenando el modelo SVC final en TODOS los datos de X_train...\n",
      "Entrenamiento final completado.\n",
      "\n",
      "Evaluación Final del Modelo SVC en X_test\n",
      "Primeras 10 predicciones (y_pred):\n",
      "[0 0 1 0 0 0 0 0 0 0]\n",
      "\n",
      "Primeras 10 probabilidades (y_prob) para la clase 1:\n",
      "[0.274 0.03  0.978 0.448 0.041 0.054 0.092 0.442 0.247 0.111]\n",
      "\n",
      "Evaluación con 'y_prob':\n",
      "AUC (Area Under Curve): 0.9130387063824519\n",
      "\n",
      "Evaluación con 'y_pred' (umbral 0.5):\n",
      "Accuracy (Exactitud): 0.8608935974205435\n",
      "\n",
      "Reporte de Clasificación (Precisión, Recall, etc.):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91      4945\n",
      "           1       0.78      0.59      0.67      1568\n",
      "\n",
      "    accuracy                           0.86      6513\n",
      "   macro avg       0.83      0.77      0.79      6513\n",
      "weighted avg       0.86      0.86      0.85      6513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_params_svc = study_svc.best_params\n",
    "print(\"Mejores hiperparámetros encontrados: \" + str(best_params_svc))\n",
    "\n",
    "final_svc_model = SVC(\n",
    "    kernel='rbf',\n",
    "    C=best_params_svc['C'],         \n",
    "    gamma=best_params_svc['gamma'], \n",
    "    probability=True,                \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline final\n",
    "final_pipeline_svc = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', final_svc_model)\n",
    "])\n",
    "\n",
    "# modelo final con todos los datos de X_train\n",
    "print(\"Entrenando el modelo SVC final en TODOS los datos de X_train...\")\n",
    "final_pipeline_svc.fit(X_train, y_train)\n",
    "print(\"Entrenamiento final completado.\")\n",
    "\n",
    "print(\"\\nEvaluación Final del Modelo SVC en X_test\")\n",
    "\n",
    "# Obteniendo y_pred (Predicciones de Clase 0 o 1)\n",
    "y_pred_svc = final_pipeline_svc.predict(X_test)\n",
    "print(\"Primeras 10 predicciones (y_pred):\")\n",
    "print(str(y_pred_svc[:10]))\n",
    "\n",
    "# Obteniendo y_prob (Predicciones de Probabilidad) \n",
    "y_prob_svc = final_pipeline_svc.predict_proba(X_test)[:, 1]\n",
    "print(\"\\nPrimeras 10 probabilidades (y_prob) para la clase 1:\")\n",
    "print(str(np.round(y_prob_svc[:10], 3)))\n",
    "\n",
    "# Usando y_pred y y_prob para Evaluación \n",
    "auc_svc = roc_auc_score(y_test, y_prob_svc)\n",
    "print(\"\\nEvaluación con 'y_prob':\")\n",
    "print(\"AUC (Area Under Curve): \" + str(auc_svc))\n",
    "\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "report_svc = classification_report(y_test, y_pred_svc)\n",
    "\n",
    "print(\"\\nEvaluación con 'y_pred' (umbral 0.5):\")\n",
    "print(\"Accuracy (Exactitud): \" + str(accuracy_svc))\n",
    "print(\"\\nReporte de Clasificación (Precisión, Recall, etc.):\")\n",
    "print(str(report_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580bb372",
   "metadata": {},
   "source": [
    "## Interpretación\n",
    "El modelo es excelente para discriminar entre las dos clases. Si tomamos a una persona al azar que gana >50K (clase 1) y otra que gana <=50K (clase 0), hay un 91.3% de probabilidad de que el modelo le asigne una probabilidad más alta a la persona correcta.\n",
    "\n",
    "**Accuracy:** 0.860 El 86% de tus predicciones totales en el set de prueba fueron correctas.\n",
    "\n",
    "**Clase 0 (<=50K, la mayoría):** El modelo es excepcional prediciendo a la gente que gana poco. Los encuentra (recall del 95%) y acierta cuando lo dice (precision del 88%).\n",
    "\n",
    "**Clase 1 (>50K, la minoría):** Cuando el modelo dice \"1\" (>50K), acierta el 78% de las veces (precision). El modelo solo logra encontrar al 59% de todas las personas que realmente ganan >50K (recall). El otro 41% los clasifica mal como \"0\" (son Falsos Negativos).\n",
    "\n",
    "**Conclusión:** Igual que en el anterior, si el objetivo del modelo fuera encontrar todos los posibles clientes de bajos ingresos (Clase 0), funciona perfectamente, ya que tiene un Recall del 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac23d7f",
   "metadata": {},
   "source": [
    "## Perceptrón Multicapas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3850896c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-02 14:46:54,547] A new study created in memory with name: no-name-48775b59-d64b-4b3b-90bb-1bb59e3f874e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Iniciando Optimización: Multi-layer Perceptron (MLP) ---\n",
      "Iniciando estudio (n_trials=15) usando un sampler GP...\n",
      "ADVERTENCIA: MLP es lento de entrenar, esto puede tardar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-02 14:48:19,117] Trial 0 finished with value: 0.9090776841686783 and parameters: {'n_layers': 1, 'n_units_l1': 112, 'alpha': 0.042908046131693675}. Best is trial 0 with value: 0.9090776841686783.\n",
      "[I 2025-11-02 14:50:54,665] Trial 1 finished with value: 0.90065495323465 and parameters: {'n_layers': 1, 'n_units_l1': 71, 'alpha': 0.007889941443174295}. Best is trial 0 with value: 0.9090776841686783.\n",
      "[I 2025-11-02 14:54:36,207] Trial 2 finished with value: 0.9016248764060606 and parameters: {'n_layers': 1, 'n_units_l1': 80, 'alpha': 0.011589586267693797}. Best is trial 0 with value: 0.9090776841686783.\n",
      "[I 2025-11-02 15:00:29,674] Trial 3 finished with value: 0.8869519104705509 and parameters: {'n_layers': 2, 'n_units_l1': 29, 'n_units_l2': 76, 'alpha': 0.0006490191209409074}. Best is trial 0 with value: 0.9090776841686783.\n",
      "[I 2025-11-02 15:03:49,605] Trial 4 finished with value: 0.8940975515602352 and parameters: {'n_layers': 2, 'n_units_l1': 27, 'n_units_l2': 84, 'alpha': 0.008835085192268985}. Best is trial 0 with value: 0.9090776841686783.\n",
      "[I 2025-11-02 15:04:28,972] Trial 5 finished with value: 0.9121987208352408 and parameters: {'n_layers': 1, 'n_units_l1': 21, 'alpha': 0.07781732738330753}. Best is trial 5 with value: 0.9121987208352408.\n",
      "[I 2025-11-02 15:10:04,591] Trial 6 finished with value: 0.8805826482793009 and parameters: {'n_layers': 2, 'n_units_l1': 80, 'n_units_l2': 107, 'alpha': 0.007889644326841052}. Best is trial 5 with value: 0.9121987208352408.\n",
      "[I 2025-11-02 15:13:27,265] Trial 7 finished with value: 0.8783081952296945 and parameters: {'n_layers': 2, 'n_units_l1': 72, 'n_units_l2': 29, 'alpha': 2.740968215273379e-05}. Best is trial 5 with value: 0.9121987208352408.\n",
      "[I 2025-11-02 15:14:33,507] Trial 8 finished with value: 0.9108775145525293 and parameters: {'n_layers': 1, 'n_units_l1': 92, 'alpha': 0.06362971124232047}. Best is trial 5 with value: 0.9121987208352408.\n",
      "[I 2025-11-02 15:17:20,803] Trial 9 finished with value: 0.889621875623259 and parameters: {'n_layers': 1, 'n_units_l1': 87, 'alpha': 1.990150181908391e-05}. Best is trial 5 with value: 0.9121987208352408.\n",
      "[I 2025-11-02 15:18:22,234] Trial 10 finished with value: 0.9130929482471574 and parameters: {'n_layers': 1, 'n_units_l1': 128, 'alpha': 0.09927146014931387}. Best is trial 10 with value: 0.9130929482471574.\n",
      "[I 2025-11-02 15:18:57,361] Trial 11 finished with value: 0.9127162925885692 and parameters: {'n_layers': 1, 'n_units_l1': 51, 'alpha': 0.1}. Best is trial 10 with value: 0.9130929482471574.\n",
      "[I 2025-11-02 15:19:46,437] Trial 12 finished with value: 0.9126662110675984 and parameters: {'n_layers': 1, 'n_units_l1': 109, 'alpha': 0.1}. Best is trial 10 with value: 0.9130929482471574.\n",
      "[I 2025-11-02 15:20:06,977] Trial 13 finished with value: 0.9120475910833035 and parameters: {'n_layers': 1, 'n_units_l1': 16, 'alpha': 0.09728201322297539}. Best is trial 10 with value: 0.9130929482471574.\n",
      "[I 2025-11-02 15:21:10,699] Trial 14 finished with value: 0.9130929482471574 and parameters: {'n_layers': 1, 'n_units_l1': 128, 'alpha': 0.09927146014931387}. Best is trial 10 with value: 0.9130929482471574.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimización de MLP con GP Completada\n",
      "Mejor score (AUC promedio): 0.9130929482471574\n",
      "Mejores Hiperparámetros: {'n_layers': 1, 'n_units_l1': 128, 'alpha': 0.09927146014931387}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- 3. Iniciando Optimización: Multi-layer Perceptron (MLP) ---\")\n",
    "\n",
    "def objective_mlp(trial):\n",
    "    # 1. Probará con 1 o 2 capas ocultas\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 2)\n",
    "    \n",
    "    # 2. Definir neuronas por capa\n",
    "    hidden_layers = []\n",
    "    for i in range(n_layers):\n",
    "        # Sugerir un número de neuronas \n",
    "        n_units = trial.suggest_int('n_units_l' + str(i+1), 16, 128)\n",
    "        hidden_layers.append(n_units)\n",
    "    \n",
    "    # Convertir a tupla \n",
    "    hidden_layers_tuple = tuple(hidden_layers)\n",
    "    \n",
    "    # 3. Sugerir 'alpha' (regularización L2) en escala logarítmica\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    \n",
    "    # Creamos el modelo \n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layers_tuple,\n",
    "        alpha=alpha,\n",
    "        max_iter=1000,\n",
    "        learning_rate_init=0.001,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Pipeline completo \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Validamos con K-Fold y calculamos el AUC \n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "    \n",
    "    # Devolvemos el AUC promedio \n",
    "    auc_promedio = scores.mean()\n",
    "    return auc_promedio\n",
    "\n",
    "# Iniciar el estudio de Optuna con GPSampler \n",
    "n_trials_mlp = 15\n",
    "print(\"Iniciando estudio (n_trials=\" + str(n_trials_mlp) + \") usando un sampler GP...\")\n",
    "\n",
    "sampler = GPSampler() \n",
    "study_mlp = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "# Ejecutamos la optimización\n",
    "study_mlp.optimize(objective_mlp, n_trials=n_trials_mlp)\n",
    "\n",
    "# Resultados \n",
    "print(\"\\nOptimización de MLP con GP Completada\")\n",
    "print(\"Mejor score (AUC promedio): \" + str(study_mlp.best_value))\n",
    "print(\"Mejores Hiperparámetros: \" + str(study_mlp.best_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c286157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros encontrados: {'n_layers': 1, 'n_units_l1': 128, 'alpha': 0.09927146014931387}\n",
      "Arquitectura de red final: (128,)\n",
      "Entrenando el modelo MLP final en TODOS los datos de X_train...\n",
      "Entrenamiento final completado.\n",
      "Primeras 10 predicciones (y_pred):\n",
      "[0 0 1 0 0 0 0 1 0 0]\n",
      "\n",
      "Primeras 10 probabilidades (y_prob) para la clase 1:\n",
      "[0.172 0.002 0.95  0.468 0.079 0.015 0.024 0.687 0.318 0.071]\n",
      "\n",
      "Evaluación con 'y_prob':\n",
      "AUC (Area Under Curve): 0.9188055859350819\n",
      "\n",
      "Evaluación con 'y_pred' (umbral 0.5):\n",
      "Accuracy (Exactitud): 0.8622754491017964\n",
      "\n",
      "Reporte de Clasificación (Precisión, Recall, etc.):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91      4945\n",
      "           1       0.75      0.64      0.69      1568\n",
      "\n",
      "    accuracy                           0.86      6513\n",
      "   macro avg       0.82      0.79      0.80      6513\n",
      "weighted avg       0.86      0.86      0.86      6513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_params_mlp = study_mlp.best_params\n",
    "print(\"Mejores hiperparámetros encontrados: \" + str(best_params_mlp))\n",
    "\n",
    "# 2. Re-construir la tupla de capas ocultas\n",
    "best_layers = []\n",
    "for i in range(best_params_mlp['n_layers']):\n",
    "    best_layers.append(best_params_mlp['n_units_l' + str(i+1)])\n",
    "best_layers_tuple = tuple(best_layers)\n",
    "print(\"Arquitectura de red final: \" + str(best_layers_tuple))\n",
    "\n",
    "# 3. Crear el modelo MLP final con esa receta\n",
    "final_mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=best_layers_tuple, \n",
    "    alpha=best_params_mlp['alpha'],      \n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Crear el pipeline final\n",
    "final_pipeline_mlp = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', final_mlp_model)\n",
    "])\n",
    "\n",
    "# 5. Entrenar el modelo final con los datos de X_train\n",
    "print(\"Entrenando el modelo MLP final en TODOS los datos de X_train...\")\n",
    "final_pipeline_mlp.fit(X_train, y_train)\n",
    "print(\"Entrenamiento final completado.\")\n",
    "\n",
    "# 1. Obteniendo y_pred (Predicciones de Clase 0 o 1) \n",
    "y_pred_mlp = final_pipeline_mlp.predict(X_test)\n",
    "print(\"Primeras 10 predicciones (y_pred):\")\n",
    "print(str(y_pred_mlp[:10]))\n",
    "\n",
    "# 2. Obteniendo y_prob (Predicciones de Probabilidad)\n",
    "y_prob_mlp = final_pipeline_mlp.predict_proba(X_test)[:, 1]\n",
    "print(\"\\nPrimeras 10 probabilidades (y_prob) para la clase 1:\")\n",
    "print(str(np.round(y_prob_mlp[:10], 3)))\n",
    "\n",
    "# 3. Usando y_pred y y_prob para Evaluación ---\n",
    "auc_mlp = roc_auc_score(y_test, y_prob_mlp)\n",
    "print(\"\\nEvaluación con 'y_prob':\")\n",
    "print(\"AUC (Area Under Curve): \" + str(auc_mlp))\n",
    "\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "report_mlp = classification_report(y_test, y_pred_mlp)\n",
    "\n",
    "print(\"\\nEvaluación con 'y_pred' (umbral 0.5):\")\n",
    "print(\"Accuracy (Exactitud): \" + str(accuracy_mlp))\n",
    "print(\"\\nReporte de Clasificación (Precisión, Recall, etc.):\")\n",
    "print(str(report_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d575bd",
   "metadata": {},
   "source": [
    "### **Interpretación:**\n",
    "\n",
    "El modelo es muy bueno para discriminar entre las dos clases y es el mejor modelo de los tres que has probado. Si tomamos a una persona al azar que gana >50K (clase 1) y otra que gana <=50K (clase 0), hay un 91.9% de probabilidad de que el modelo le asigne una probabilidad más alta a la persona correcta.\n",
    "\n",
    "**AUC (Area Under Curve):** 0.9188 Este es el mejor AUC (superando al 0.913 del SVC y al 0.903 de la Regresión Logística). La optimización bayesiana encontró una arquitectura de red (una capa oculta con 128 neuronas) que es muy poderosa para este problema.\n",
    "\n",
    "**Accuracy:** 0.862 El 86.2% de tus predicciones totales en el set de prueba fueron correctas. Este es también tu mejor Accuracy general.\n",
    "\n",
    "**Clase 0 (<=50K, la mayoría):** El modelo es excelente prediciendo a la gente que gana poco. Los encuentra (recall del 93%) y acierta cuando lo dice (precision del 89%).\n",
    "\n",
    "**Clase 1 (>50K, la minoría):** Cuando el modelo dice \"1\" (>50K), acierta el 75% de las veces (precision). El modelo logra encontrar al 64% de todas las personas que realmente ganan >50K (recall). El otro 36% los clasifica mal como \"0\" (son Falsos Negativos). Este recall es una leve mejora sobre el 59% del SVC.\n",
    "\n",
    "**Conclusión:** Este MLP es el modelo campeón. Si el objetivo del modelo fuera encontrar todos los posibles clientes de bajos ingresos (Clase 0), funciona perfectamente, con un Recall del 93%.\n",
    "\n",
    "Si fuera lo contrario (encontrar a los de Clase 1), su recall del 64% es mejor, pero aún habría que bajar el umbral para encontrar a más. Esto se debe a que el dataset es desbalanceado (hay muchas más personas \"0\" que \"1\", 4945 vs 1568). El modelo aprende que \"decir 0\" es una apuesta segura para subir el Accuracy general. La mayoría de los problemas del mundo real (detección de fraude, diagnósticos médicos, etc.) son desbalanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc93b7",
   "metadata": {},
   "source": [
    "**Prueba bajando el umbral para que identifique mejor a la clase 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "760ef313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajustando el Umbral de Decisión del MLP ---\n",
      "Umbral por defecto: 0.5\n",
      "Nuevo umbral elegido: 0.35\n",
      "\n",
      "Primeras 10 probabilidades (y_prob):\n",
      "[0.172 0.002 0.95  0.468 0.079 0.015 0.024 0.687 0.318 0.071]\n",
      "Primeras 10 predicciones (y_pred) con umbral 0.5:\n",
      "[0 0 1 0 0 0 0 1 0 0]\n",
      "Primeras 10 predicciones (y_pred) con umbral 0.35:\n",
      "[0 0 1 1 0 0 0 1 0 0]\n",
      "\n",
      "--- Evaluación con NUEVO Umbral (0.35) ---\n",
      "Accuracy (Exactitud) con nuevo umbral: 0.8466144633809305\n",
      "\n",
      "Reporte de Clasificación (Precisión, Recall, etc.) con nuevo umbral:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90      4945\n",
      "           1       0.65      0.79      0.71      1568\n",
      "\n",
      "    accuracy                           0.85      6513\n",
      "   macro avg       0.79      0.83      0.80      6513\n",
      "weighted avg       0.86      0.85      0.85      6513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Ajustando el Umbral de Decisión del MLP ---\")\n",
    "\n",
    "nuevo_umbral = 0.35\n",
    "print(\"Umbral por defecto: 0.5\")\n",
    "print(\"Nuevo umbral elegido: \" + str(nuevo_umbral))\n",
    "\n",
    "# Aplicamos el nuevo umbral a las probabilidades ---\n",
    "# (y_prob_mlp > nuevo_umbral) crea un array de True/False\n",
    "# .astype(int) convierte True->1 y False->0\n",
    "y_pred_nuevo = (y_prob_mlp > nuevo_umbral).astype(int)\n",
    "\n",
    "print(\"\\nPrimeras 10 probabilidades (y_prob):\")\n",
    "print(str(np.round(y_prob_mlp[:10], 3)))\n",
    "print(\"Primeras 10 predicciones (y_pred) con umbral 0.5:\")\n",
    "print(str(y_pred_mlp[:10])) \n",
    "print(\"Primeras 10 predicciones (y_pred) con umbral \" + str(nuevo_umbral) + \":\")\n",
    "print(str(y_pred_nuevo[:10]))\n",
    "\n",
    "# Evaluamos el rendimiento CON EL NUEVO UMBRAL\n",
    "print(\"\\n--- Evaluación con NUEVO Umbral (\" + str(nuevo_umbral) + \") ---\")\n",
    "\n",
    "accuracy_nuevo = accuracy_score(y_test, y_pred_nuevo)\n",
    "report_nuevo = classification_report(y_test, y_pred_nuevo)\n",
    "\n",
    "print(\"Accuracy (Exactitud) con nuevo umbral: \" + str(accuracy_nuevo))\n",
    "print(\"\\nReporte de Clasificación (Precisión, Recall, etc.) con nuevo umbral:\")\n",
    "print(str(report_nuevo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d575b60",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusiones\n",
    "\n",
    "**Antes (Umbral 0.5):** Recall Clase 1 = 0.64 \n",
    "\n",
    "**Ahora (Umbral 0.35):** Recall Clase 1 = 0.79 \n",
    "\n",
    "Bajar el umbral hizo que el modelo lograra identificar a un 15% más del grupo de altos ingresos que antes estaba omitiendo.\n",
    "\n",
    "¿Cuál fue el \"costo\" de este cambio?\n",
    "\n",
    "**Precisión Clase 1:** Bajó de 0.75 a 0.65.\n",
    "\n",
    "Para poder encontrar a más personas (subir el Recall), el modelo ahora comete más Falsos Positivos. Antes, el 75% de sus predicciones \"1\" eran correctas; ahora, solo el 65% lo son.\n",
    "\n",
    "**Accuracy Total:** Bajó ligeramente de 0.862 a 0.847.\n",
    "\n",
    "El Accuracy general bajó un poco porque el modelo ahora se equivoca más con la Clase 0 (que es la mayoría) para poder acertar más con la Clase 1 (la minoría)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
